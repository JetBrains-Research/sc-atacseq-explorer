{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single cell ATAC-seq explorer\n",
    "\n",
    "This single cell ATAC-Seq analysis pipeline is designed for advanced analysis of\n",
    "dataset, produced by 10X Genomics Cell Ranger ATAC.\\\n",
    "Aggregated datasets are also supported!\n",
    "\n",
    "**In addition** to 10X Genomics results it offers:\n",
    "\n",
    "* Capable to process aggregated data by 10x Genomics Cell Ranger ATAC\n",
    "* Summary on different conditions in case of aggregated dataset\n",
    "* Different types of clustering followed by heatmap and t-SNE/UMAP visualizations in low dimensions space\n",
    "* Top cluster markers visualization as heatmap on t-SNE/UMAP plot\n",
    "* Closest genes annotations for peaks and clusters\n",
    "* Annotated markers analysis\n",
    "* BigWig and BED files for clusters and markers ready-to-be-visualized in [JBR Genome Browser](https://research.jetbrains.org/groups/biolabs/tools/jbr-genome-browser) by JetBrains Research\n",
    "* Data preparation for [Single Cell Explorer](http://artyomovlab.wustl.edu/sce/) by Artyomov Lab, Washington University in St.Louis\n",
    "* Save all the figures in PDF format - ready for publication\n",
    "\n",
    "Required 10X Genomics Cell Ranger ATAC files:\n",
    "\n",
    "* `fragments.tsv` - fragments matrix file provided by Cell Ranger\n",
    "* `peaks.bed` - peaks file (union of peaks) in case of merging\n",
    "* `clusters.csv` - Graph clustering after median normalization, IDF scaling, SVD projection and L2 normalization\n",
    "* `projection.csv` - t-SNE 2d project of all the cells\n",
    "\n",
    "Pipeline steps:\n",
    "\n",
    "* Cell Calling\n",
    "* Peak barcode matrix\n",
    "* Feature selection\n",
    "* Dimensionality reduction\n",
    "* t-SNE / UMAP\n",
    "* Differential markers analysis\n",
    "* Closest genes association\n",
    "* Supervised annotation of clusters by gene markers\n",
    "* Export data to Single Cell Explorer format\n",
    "\n",
    "Other pipelines:\n",
    "\n",
    "* **Cell Ranger ATAC-Seq** https://support.10xgenomics.com/single-cell-atac/software/pipelines/latest/algorithms/overview\n",
    "* **Seurat** https://www.biorxiv.org/content/biorxiv/early/2018/11/02/460147.full.pdf (scRNA-seq only https://github.com/satijalab/seurat/issues/1422)\n",
    "* **Scasat** https://academic.oup.com/nar/article/47/2/e10/5134327\n",
    "* **SnapATAC** https://www.biorxiv.org/content/10.1101/615179v2\n",
    "* **Scater** https://academic.oup.com/bioinformatics/article/33/8/1179/2907823\n",
    "* **Signac** https://satijalab.org/signac/articles/pbmc_vignette.html\n",
    "* Single Cell ATAC benchmark https://github.com/pinellolab/scATAC-benchmarking\n",
    "\n",
    "\n",
    "Source code is available at: https://github.com/JetBrains-Research/sc-atacseq-explorer \\\n",
    "Questions and comments are welcome at os at jetbrains dot com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline paths configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration of 10X Genomics files\n",
    "FRAGMENTS_FILE_10X = 'fragments.tsv.gz'\n",
    "CLUSTERS_FILE_10X = 'clusters.csv'\n",
    "TSNE_FILE_10X = 'tsne.csv'\n",
    "PEAKS_FILE = 'peaks.bed'\n",
    "\n",
    "# See https://www.encodeproject.org/annotations/ENCSR636HFF/\n",
    "# Set this value to None if blacklist regions bed file is not available\n",
    "BLACKLIST_FILE = 'blacklist.bed'\n",
    "\n",
    "# For mm10 and hg38\n",
    "# ! wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M25/gencode.vM25.annotation.gtf.gz\n",
    "# ! wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_32/gencode.v32.annotation.gtf.gz\n",
    "# ! gunzip *.gtf.gz\n",
    "GTF_FILE = 'gencodegenes.gtf'\n",
    "\n",
    "# Dowload DNAse hypersensitivity sites from encodeproject.org\n",
    "# For mm10\n",
    "# ! wget https://www.encodeproject.org/files/ENCFF108APF/@@download/ENCFF108APF.bed.gz\n",
    "# ! gunzip *.gz\n",
    "# Set this to None if DNASe file is not available\n",
    "DNASE_FILE = 'dnase.bed'\n",
    "\n",
    "\n",
    "# Output configuration\n",
    "OUTPUT_DIR = 'pipeline'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline parameters configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factors configuration, number should be consistent with number of merged \n",
    "FACTOR = 'factor'\n",
    "FACTORS_MAP = {1: 'f1', 2: 'f2'}\n",
    "\n",
    "# Aggregated barcodes always have suffix as a marker of original track\n",
    "def FACTOR_FUNCTION(x):\n",
    "    for k in [1, 2]:\n",
    "        if x.endswith(f'-{k}'):\n",
    "            return k\n",
    "\n",
    "# Cell calling filtration thresholds\n",
    "NOISE_THRESHOLD = 200\n",
    "DUPLETS_THRESHOLD = 8000\n",
    "\n",
    "# Default by Cell Ranger ATAC\n",
    "LSA_COMPONENTS = 15\n",
    "\n",
    "# 20 clusters is default by Cell Ranger ATAC\n",
    "N_CLUSTERS = 20\n",
    "        \n",
    "# Markers configuration\n",
    "MARKERS_GENES = []\n",
    "\n",
    "# Export to Single Cell Explorer configuration\n",
    "SCE_TOKEN = '2020_scatacseq'\n",
    "SCE_NAME = 'Single cell ATAC-Seq'\n",
    "SCE_PEAK_GENE_TSS_MAX_DISTANCE = 10000\n",
    "SCE_PUBLIC = False\n",
    "SCE_ORGANISM = 'mm' # mm or hg are supported\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all the libraries are installed\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format ='retina'\n",
    "\n",
    "import gc\n",
    "import glob\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# Workaround for the issue: https://github.com/pandas-dev/pandas/issues/26314\n",
    "# pip install pandas==0.21\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import umap\n",
    "sns.set_style(\"whitegrid\")\n",
    "import re\n",
    "import tempfile\n",
    "import glob\n",
    "import pyBigWig\n",
    "import json\n",
    "import h5py\n",
    "from scipy import io, sparse\n",
    "from scipy.cluster import hierarchy\n",
    "\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from collections import Counter\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "from pybedtools import BedTool\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.stats import zscore\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# These functions are taken from the original Cell Ranger 10x ATAC pipeline\n",
    "# See https://github.com/10XGenomics/cellranger/\n",
    "from irlb import irlb\n",
    "from diffexp import *\n",
    "\n",
    "# Fix random seed\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline output folders\n",
    "figures_path = os.path.join(OUTPUT_DIR, 'figures')\n",
    "! mkdir -p {figures_path}\n",
    "! mkdir -p {OUTPUT_DIR}/cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used memory analysis utility\n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "# Cleanup\n",
    "gc.collect()\n",
    "\n",
    "# Print all allocated variables\n",
    "def print_mem_usage():\n",
    "    for name, size in sorted(((name, sys.getsizeof(value)) for name, value in globals().items()),\n",
    "                             key= lambda x: x[1],\n",
    "                             reverse=True)[:10]:\n",
    "        print(\"Global {:>30}: {:>8}\".format(name, sizeof_fmt(size)))    \n",
    "    for name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()),\n",
    "                             key= lambda x: x[1],\n",
    "                             reverse=True)[:10]:\n",
    "        print(\"Local {:>30}: {:>8}\".format(name, sizeof_fmt(size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize 10x Cell Ranger results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters10xdf = pd.read_csv(CLUSTERS_FILE_10X, sep=',')\n",
    "clusters10xdf['Cluster'] = clusters10xdf['Cluster'].astype(int) - 1 # Clusters start with 1 in 10x\n",
    "clusters10xdf[FACTOR] = [FACTOR_FUNCTION(bc) for bc in clusters10xdf['Barcode']]\n",
    "# display(clusters10xdf.head())\n",
    "print('Total clusters', len(set(clusters10xdf['Cluster'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusters_sizes(df, value):\n",
    "    df_sum = df[['cluster', value]].groupby(['cluster']).sum().reset_index()\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    cmap=plt.cm.get_cmap('jet', len(df_sum))\n",
    "    sns.barplot(x=df_sum.index, y=df_sum[value],\n",
    "                palette=[cmap(i) for i in range(len(df_sum))])\n",
    "\n",
    "\n",
    "def clusters_factors(df, value, FACTORS_MAP):\n",
    "    df_sum = df[['cluster', value]].groupby(['cluster']).sum().reset_index()    \n",
    "    df_sum_factor = df[['cluster', FACTOR, value]].groupby(['cluster', FACTOR]).sum().reset_index()\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    cmap=plt.cm.get_cmap('jet', len(FACTORS_MAP))\n",
    "    \n",
    "    bottom = np.zeros(len(df_sum))\n",
    "    for i, (k,v) in enumerate(FACTORS_MAP.items()):            \n",
    "            heights = np.array(df_sum_factor.loc[df_sum_factor[FACTOR] == k][value]) / np.array(df_sum[value]) * 100.0\n",
    "            plt.bar(x = df_sum.index, height = heights, bottom=bottom, color = cmap(i), width=0.8)\n",
    "            bottom += heights\n",
    "\n",
    "    # Cosmetics\n",
    "    plt.xticks(df_sum.index)\n",
    "    plt.gca().xaxis.grid(False)\n",
    "    plt.gca().margins(x=0.005)\n",
    "    \n",
    "    legend_colors = [plt.Rectangle((0,0), 1, 1, fc=cmap(i), edgecolor = 'none') \n",
    "                         for i in range(len(FACTORS_MAP))]\n",
    "    legend_names = [v for _,v in FACTORS_MAP.items()]\n",
    "    l = plt.legend(legend_colors, legend_names, prop={'size': 10}, bbox_to_anchor=(1.01, 0.99))\n",
    "    l.draw_frame(False)\n",
    "\n",
    "    plt.ylabel('Cluster composition ' + FACTOR + ' %')\n",
    "    plt.xlabel('Cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Size of clusters analysis')\n",
    "t = clusters10xdf.copy()\n",
    "t['counts'] = np.ones(len(t))\n",
    "t.rename(columns={'Cluster': 'cluster'}, inplace=True)\n",
    "t.index = t['Barcode']\n",
    "print('Summary by', FACTOR, FACTORS_MAP)\n",
    "display(t[[FACTOR, 'counts']].groupby([FACTOR]).sum())\n",
    "\n",
    "clusters_sizes(t[['cluster', 'counts', FACTOR]], 'counts')\n",
    "plt.show()\n",
    "\n",
    "clusters_factors(t[['cluster', 'counts', FACTOR]], 'counts', FACTORS_MAP)\n",
    "plt.show()\n",
    "\n",
    "# Cleanup memory\n",
    "del t\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_colored(data, c,\n",
    "                 clusters=False, show_centers=False, \n",
    "                 size=10, alpha=0.5, no_ticks=True, fig_ax=None):\n",
    "    \"\"\" Plot colored scatterplot \"\"\"\n",
    "    cmap = plt.cm.get_cmap('jet', len(set(c))) if clusters else plt.cm.viridis\n",
    "    if fig_ax is not None:\n",
    "        fig, ax = fig_ax\n",
    "    else:\n",
    "        fig, ax = plt.subplots(1, figsize=(10, 8))\n",
    "    sc = ax.scatter(data[:, 0], data[:, 1], \n",
    "                    c=c, cmap=cmap,\n",
    "                    marker='o',\n",
    "                    alpha=alpha,\n",
    "                    s=size)\n",
    "    mesh = ax.get_children()[0]\n",
    "    if no_ticks:\n",
    "        plt.setp(ax, xticks=[], yticks=[])\n",
    "    if clusters and show_centers:\n",
    "        # For each cluster, we add a text with name\n",
    "        for cluster in set(c):\n",
    "            cluster_mean = data[np.flatnonzero(c == cluster), :].mean(axis=0)\n",
    "            ax.text(cluster_mean[0], cluster_mean[1], str(cluster), \n",
    "                     horizontalalignment='center', size='large', color='black', weight='bold')            \n",
    "    if clusters:\n",
    "        cbar = plt.colorbar(mesh, ax=ax, boundaries=np.arange(len(set(c)) + 1) + min(c) - 0.5)\n",
    "        cbar.set_ticks(np.arange(len(set(c))) + min(c))\n",
    "        cbar.set_ticklabels(list(set(c)))\n",
    "    else:\n",
    "        plt.colorbar(mesh, ax=ax)\n",
    "\n",
    "def n_rows_columns(n, maxcols=5):\n",
    "    ncols = max([r for r in range(1, min(maxcols, math.ceil(math.sqrt(n)) + 1)) if n % r == 0])\n",
    "    nrows = int(n / ncols)\n",
    "    return nrows, ncols\n",
    "\n",
    "    \n",
    "def plot_colored_split(data, c, size=10, alpha=0.5, contrast = 100):\n",
    "    \"\"\" Plot colored scatterplot split by factors\"\"\"\n",
    "    cmap = plt.cm.get_cmap('jet', len(set(c)))\n",
    "    nrows, ncols = n_rows_columns(len(set(c)))\n",
    "    plt.subplots(nrows=nrows, ncols=ncols, figsize=(5 * ncols, 5 * nrows))\n",
    "    xmin, xmax = np.min(data[:,0]), np.max(data[:,0])\n",
    "    xlim = (xmin - 0.1 * (xmax - xmin), xmax + 0.1 * (xmax - xmin)) # +10% margin\n",
    "    ymin, ymax = np.min(data[:,1]), np.max(data[:,1])\n",
    "    ylim = (ymin - 0.1 * (ymax - ymin), ymax + 0.1 * (ymax - ymin)) # +10% margin\n",
    "    for i, f in enumerate(tqdm(set(c))):\n",
    "        ax = plt.subplot(nrows, ncols, i + 1)\n",
    "        plt.setp(ax, xticks=[], yticks=[])\n",
    "        plt.xlim(xlim)\n",
    "        plt.ylim(ylim)\n",
    "        plt.title(str(f))\n",
    "        # Plot slightly visible other as gray\n",
    "        nfdata = data[np.flatnonzero(np.logical_not(np.equal(c, f))), :]\n",
    "        plt.scatter(nfdata[:,0], nfdata[:,1], \n",
    "            color='gray',\n",
    "            marker='o',\n",
    "            alpha=alpha / contrast,\n",
    "            s=size)\n",
    "        # And factor data\n",
    "        fdata = data[np.flatnonzero(np.equal(c, f)), :]        \n",
    "        plt.scatter(fdata[:,0], fdata[:,1], \n",
    "                    color=cmap(i),\n",
    "                    marker='o',\n",
    "                    alpha=alpha,\n",
    "                    s=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cell Ranger ATAC original T-SNE visualization')\n",
    "tsne10xpath = TSNE_FILE_10X\n",
    "tsne10xdf = pd.read_csv(tsne10xpath, sep=',')\n",
    "\n",
    "mergeddf = pd.merge(left=tsne10xdf,\n",
    "                    right=clusters10xdf, \n",
    "                    left_on='Barcode', right_on='Barcode')\n",
    "\n",
    "plot_colored(mergeddf[['TSNE-1', 'TSNE-2']].values, mergeddf['Cluster'], \n",
    "             clusters=True,\n",
    "             show_centers=True,\n",
    "             size=20, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('tSNE on LSA normalized vs', FACTOR, FACTORS_MAP)\n",
    "plot_colored(mergeddf[['TSNE-1', 'TSNE-2']].values, clusters10xdf[FACTOR], clusters=True, size=20, alpha=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Uncomment to plot split by factor\n",
    "# with PdfPages(os.path.join(figures_path, '10x_tsne_clusters_split.pdf')) as pdf:\n",
    "#     plot_colored_split(mergeddf[['TSNE-1', 'TSNE-2']].values, clusters10xdf[FACTOR], size=5, alpha=0.1)\n",
    "#     pdf.savefig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peaks file stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with PdfPages(os.path.join(figures_path, 'peaks_length.pdf')) as pdf:\n",
    "    peaks_df = pd.read_csv(PEAKS_FILE, sep='\\t', header=None)\n",
    "    print('Peaks', len(peaks_df))\n",
    "    sns.distplot(np.log10(peaks_df[2] - peaks_df[1]))\n",
    "    plt.title('Peak length distribution')\n",
    "    plt.xlabel('Log10 Length')\n",
    "    plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overlap of peaks with representative DNAse hypersensitive sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DNASE_FILE is not None:\n",
    "    dnase = BedTool(DNASE_FILE)\n",
    "    peaks_file = BedTool(PEAKS_FILE)\n",
    "    peak_count = peaks_file.count()\n",
    "    overlap = peaks_file.intersect(dnase, wa=True, u=True).count()\n",
    "    peak_by_dnase = 0 if overlap == 0 else overlap * 100.0 / peak_count\n",
    "    overlap = dnase.intersect(peaks_file, wa=True, u=True).count()\n",
    "    dnase_by_peak = overlap * 100.0 / dnase.count()\n",
    "    print('Fraction of peaks overlapping with representative DNAse', int(peak_by_dnase), '%')\n",
    "    print('Fraction of representative DNAse overlapping with peaks', int(dnase_by_peak), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotate peaks with closest genes\n",
    "\n",
    "**Pipeline** Preprocess GENES to TSS and use `bedtools closest -D b` to report distance to TSS, upstream or downstream w.r.t. the gene strand.\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Cell Ranger ATAC** We use `bedtools closest -D b` to associate each peak with genes based on closest transcription start sites (packaged within the reference) such that the peak is within 1000 bases upstream or 100 bases downstream of the TSS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Preprocess GTF file')\n",
    "\n",
    "gtf_file = GTF_FILE\n",
    "transcripts_bed = os.path.join(OUTPUT_DIR, 'cache', os.path.basename(gtf_file) + '.bed')\n",
    "if not os.path.exists(transcripts_bed):\n",
    "    GTF_TO_BED_SH = \"\"\"\n",
    "# Compute transcript name field in GTF file\n",
    "TNF=$(cat $1 | grep transcript_id | grep -v '#' | head -n 1 | awk '{for (i=4; i<NF; i++) {if ($3==\"transcript\" && $i==\"gene_name\") print (i+1)}}')\n",
    "# Process GTF file - transcripts of protein_coding genes\n",
    "cat $1 | grep 'protein_coding' | awk -v TNF=$TNF -v OFS=\"\\t\" '{if ($3==\"transcript\") {print $1,$4-1,$5,$TNF,0,$7}}' | tr -d '\";' | sort -k1,1 -k2,2n -k3,3n --unique > $2\n",
    "    \"\"\"\n",
    "    with tempfile.NamedTemporaryFile(prefix='gtf_to_bed', suffix='.sh', delete=False) as f:\n",
    "        f.write(GTF_TO_BED_SH.encode('utf-8'))\n",
    "        f.close()\n",
    "        ! bash {f.name} {gtf_file} {transcripts_bed} \n",
    "print('Transcripts bed file', transcripts_bed)\n",
    "\n",
    "transcripts_tss = os.path.join(OUTPUT_DIR, 'cache', os.path.basename(gtf_file) + '.tss')\n",
    "if not os.path.exists(transcripts_tss):\n",
    "    BED_TO_TSS_SH = \"\"\"\n",
    "cat $1 | awk -v OFS=\"\\t\" '{if ($6==\"+\") {print $1, $2, $2+1, $4, 0, \"+\"} else {print $1,$3-1,$3, $4, 0, \"-\"}}' | sort -k1,1 -k2,2n -k3,3n --unique > $2\n",
    "    \"\"\"\n",
    "    with tempfile.NamedTemporaryFile(prefix='bed_to_tss', suffix='.sh', delete=False) as f:\n",
    "        f.write(BED_TO_TSS_SH.encode('utf-8'))\n",
    "        f.close()\n",
    "        ! bash {f.name} {transcripts_bed} {transcripts_tss} \n",
    "print('Transcripts tss file', transcripts_tss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locations_closest_genes(locations, transcripts_tss):\n",
    "    print('Annotate list of locations in format \"chr:start-end\" with closest tss of genes')\n",
    "    with tempfile.NamedTemporaryFile(delete=False) as f:\n",
    "        # Save index to restore original order after sorting\n",
    "        for i, t in enumerate([re.split(':|-', p) for p in locations]):\n",
    "            f.write('{}\\t{}\\t{}\\t{}\\n'.format(*t, i).encode('utf-8'))\n",
    "        f.close()\n",
    "        closest_file = f.name + '.closest'\n",
    "        sorted_file = f.name + '.sorted'\n",
    "        ! sort -k1,1 -k2,2n {f.name} > {sorted_file}\n",
    "        ! bedtools closest -a {sorted_file} -b {transcripts_tss} -D b > {closest_file}\n",
    "        closest_df = pd.read_csv(\n",
    "            closest_file, sep='\\t', \n",
    "            names=['chr', 'start', 'end', 'index', \n",
    "                   'gene_chr', 'gene_start', 'gene_end', 'gene', 'gene_score', 'gene_strand', 'distance'])\n",
    "        # Restore original sorting as \n",
    "        closest_df.sort_values(by=['index'], inplace=True)\n",
    "        # Pick only first closest gene in case of several\n",
    "        closest_df.drop_duplicates(['index'], inplace=True)\n",
    "        return closest_df[['chr', 'start', 'end', 'gene', 'distance']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find peaks closest to markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Building genes peaks, closest peaks is mapped to gene, second closest with suffix _1, etc')\n",
    "closest_genes = locations_closest_genes(\n",
    "    peaks_df[0] + ':' + peaks_df[1].astype(str) + '-' + peaks_df[2].astype(str),\n",
    "    transcripts_tss\n",
    ")\n",
    "# Restore 'peak' to be able to merge on it\n",
    "closest_genes['peak'] = closest_genes['chr'] +\\\n",
    "                        ':' + closest_genes['start'].astype(str) +\\\n",
    "                        '-' + closest_genes['end'].astype(str)\n",
    "t = closest_genes[['gene', 'peak']].copy()\n",
    "t['distance'] = np.abs(closest_genes['distance'])\n",
    "\n",
    "# Filter peak close to gene tss of MARKERS_GENES\n",
    "marker_genes_lower = set([s.lower() for s in MARKERS_GENES])\n",
    "t = t.loc[np.logical_and(t['distance'] < 2000, [g.lower() in marker_genes_lower for g in t['gene']])]\n",
    "t.sort_values(by=['gene', 'distance'], inplace=True)\n",
    "\n",
    "marker_regions_peaks = {}\n",
    "prev_gene = None\n",
    "prev_count = 1\n",
    "seen_genes = set()\n",
    "for _, (gene, peak, distance) in t.iterrows():\n",
    "    if prev_gene != gene:\n",
    "        marker_regions_peaks[gene] = peak\n",
    "        seen_genes.add(gene.lower())\n",
    "        prev_gene = gene\n",
    "        prev_count = 1\n",
    "    elif prev_count < 3:  # Max regions per gene\n",
    "        marker_regions_peaks[f'{gene}_{prev_count}'] = peak\n",
    "        prev_count += 1\n",
    "\n",
    "print(f'Found {len(marker_regions_peaks)} marker peaks for {len(MARKERS_GENES)} peaks close to TSS')\n",
    "for g in MARKERS_GENES:\n",
    "    if g.lower() not in seen_genes:\n",
    "        print(f'Missing peak for {g}')\n",
    "for k, v in marker_regions_peaks.items():\n",
    "    print(f'{k}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# The pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Cell Calling\n",
    "\n",
    "**Pipeline** uses **noise_threshold** and **duplets_threshold**.\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Cell Ranger ATAC** aggregation: when combining data from multiple GEM groups, the cellranger-atac aggr pipeline automatically equalizes the sensitivity of the groups before merging, which is the recommended approach in order to avoid the batch effect introduced by sequencing depth. **Default method**: Subsample fragments from higher-depth GEM wells until they all have an equal number of unique fragments per cell.\n",
    "\n",
    "For each barcode:\n",
    "* we have the record of mapped high-quality fragments that passed all filters (the fragments.tsv file).\n",
    "* Having determined peaks prior to this, we use the number of fragments that overlap any peak regions.\n",
    "* Separate the signal from noise.\n",
    "\n",
    "**This works better in practice as compared to naively using the number of fragments per barcode.** \n",
    "We first subtract a depth-dependent fixed count from all barcode counts to model whitelist contamination. This fixed count is the estimated number of fragments per barcode that originated from a different GEM, assuming a contamination rate of 0.02. Then we fit a mixture model of two negative binomial distributions to capture the signal and noise. Setting an odds ratio of 1000, we separate the barcodes that correspond to real cells from the non-cell barcodes.\n",
    "\n",
    "**Scasat** If a cell has open peaks below a user defined threshold (default: 50 peaks) in the peak accessibility matrix we would remove that cell from subsequent analysis. Also peaks not observed across a user defined number of valid cells (default: 10 cells) are not considered for further downstream analysis.\n",
    "\n",
    "**SnapATAC** alternative approach -  1kb, 5kb and 10kb resolution.\n",
    "\n",
    "Existing computational methods rely on **pre-defined** regions of transposase accessibility\n",
    "identified from the aggregate signals. ...\n",
    "\n",
    "**Limitations**: \n",
    "1. It requires sufficient number of single cell profiles to create robust aggregate signal for peak calling. \n",
    "2. The cell type identification is biased toward the most abundant cell types in the tissues. \n",
    "3. These techniques lack the ability to reveal regulatory elements in the rare cell populations which are underrepresented in the aggregate signal. This concern is critical, for example, in brain tissue, where key neuron types may represent less than 1% of all cells while still playing a critical role in the neural circuit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intersect using bedtools rather than pybedtools, because they are too slow for files of this size!\n",
    "def intersect_fragments_and_peaks(fragments_file_gz, peaks_file, blacklist_file):\n",
    "    print('Fragments')\n",
    "    ! zcat {fragments_file_gz} | wc -l\n",
    "    idf = None\n",
    "    if blacklist_file is not None:\n",
    "        print('Blacklist')\n",
    "        ! wc -l {blacklist_file}\n",
    "    print('Peaks')\n",
    "    ! wc -l {peaks_file}\n",
    "\n",
    "    with tempfile.TemporaryDirectory(prefix='pipeline', dir=OUTPUT_DIR) as td:\n",
    "        print('Filtering out non-standard chromosomes')\n",
    "        chr_filtered_file = os.path.join(td, 'chromosome_filtered.bed')\n",
    "        ! zcat {fragments_file_gz} | grep -i -E 'chr[0-9mt]+[^_]' > {chr_filtered_file}\n",
    "        ! wc -l {chr_filtered_file}\n",
    "\n",
    "        if blacklist_file is not None:\n",
    "            print('Blacklist regions filtration')\n",
    "            blacklist_filtered_file = os.path.join(td, 'blacklist_filtered.bed')\n",
    "            ! bedtools intersect -v -a {chr_filtered_file} -b {blacklist_file} > {blacklist_filtered_file}\n",
    "            ! wc -l {blacklist_filtered_file}\n",
    "            # Cleanup\n",
    "            ! rm {chr_filtered_file}\n",
    "        else:\n",
    "            blacklist_filtered_file = chr_filtered_file\n",
    "\n",
    "        print('Fragments and peaks intersection')\n",
    "        intersection_file = os.path.join(td, 'intersection.bed')\n",
    "        ! bedtools intersect -wa -wb -a {blacklist_filtered_file} -b {peaks_file} > {intersection_file}\n",
    "        ! wc -l {intersection_file}\n",
    "        # Cleanup\n",
    "        ! rm {blacklist_filtered_file}\n",
    "\n",
    "        idf = pd.read_csv(intersection_file, sep='\\t', header=None)\n",
    "        # Cleanup\n",
    "        ! rm {intersection_file}        \n",
    "        icolnames = ['chr', 'start', 'end', 'barcode', 'reads', 'peak_chr', 'peak_start', 'peak_end']\n",
    "        assert len(idf.columns) >= len(icolnames)\n",
    "        idf.rename(columns={o: n for (o, n) in zip(idf.columns[:len(icolnames)], icolnames)}, inplace=True)\n",
    "        idf = idf[icolnames] # Reorder and drop others\n",
    "        idf = idf.astype({'reads': 'int8'}) # Consume less memory\n",
    "    print('Done intersecting of fragments and peaks')\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or compute fragments and peaks intersection data frame\n",
    "idfpath = os.path.join(OUTPUT_DIR, 'cache', 'idf.csv.gz')\n",
    "if os.path.exists(idfpath):\n",
    "    print(f'Loading IDF from {idfpath}')\n",
    "    idf = pd.read_csv(idfpath, compression='gzip')\n",
    "else:\n",
    "    print(f'No IDF file found {idfpath}')    \n",
    "    idf = intersect_fragments_and_peaks(FRAGMENTS_FILE_10X, PEAKS_FILE, BLACKLIST_FILE)\n",
    "    idf.to_csv(idfpath, index=False, compression='gzip')\n",
    "    print(f'Saved IDF to {idfpath}')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_cells(pidf, noise_threshold=None, duplets_threshold=None):\n",
    "    # 10x Genomics Cell Ranger ATAC-Seq marks duplicates\n",
    "    # 'count' ignores multiple reads per barcode at same position\n",
    "    pidf = pd.pivot_table(idf, values='reads', index=['barcode'], aggfunc='count')\n",
    "    pidf.reset_index(level=0, inplace=True)\n",
    "    \n",
    "    print(f'Total barcodes intersecting peaks: {len(pidf)}')    \n",
    "    plt.subplots(nrows=1, ncols=3, figsize=(20, 6))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    sns.distplot(np.log10(pidf['reads']))\n",
    "    plt.title('UMI summary coverage distribution')\n",
    "    plt.xlabel('Log10 coverage')\n",
    "    plt.ylabel('Frequency')    \n",
    "\n",
    "    # n is for shoulder graph\n",
    "    counts = sorted(pidf['reads'], reverse=True)\n",
    "    df_counts = pd.DataFrame(data={'count': counts, 'n': range(1, len(counts) + 1)})\n",
    "    cells_filter = [True] * len(df_counts)\n",
    "\n",
    "    plt.subplot(1, 3, 2)    \n",
    "    if noise_threshold is not None:\n",
    "        noise_filter = df_counts['count'] <= noise_threshold\n",
    "        cells_filter = np.logical_and(cells_filter, np.logical_not(noise_filter))\n",
    "        df_noise = df_counts.loc[noise_filter]\n",
    "        print(f'Noise <= {noise_threshold} reads per barcode: {len(df_noise)}')\n",
    "        plt.plot(np.log10(df_noise['n']), \n",
    "                 np.log10(df_noise['count']), \n",
    "                 label='Noise', linewidth=3, color='red')        \n",
    "\n",
    "    if duplets_threshold is not None:\n",
    "        duplets_filter = df_counts['count'] > duplets_threshold\n",
    "        cells_filter = np.logical_and(cells_filter, np.logical_not(duplets_filter))\n",
    "        df_duplets = df_counts.loc[duplets_filter]\n",
    "        print(f'Duplets > {duplets_threshold} reads per barcode: {len(df_duplets)}')        \n",
    "        plt.plot(np.log10(df_duplets['n']), \n",
    "                 np.log10(df_duplets['count']), \n",
    "                 label='Duplets', linewidth=3, color='orange')        \n",
    "\n",
    "    df_cells = df_counts.loc[cells_filter]\n",
    "\n",
    "    cells_filter = [True] * len(pidf)\n",
    "    if noise_threshold is not None:\n",
    "        cells_filter = np.logical_and(cells_filter, pidf['reads'] > noise_threshold)\n",
    "    if duplets_threshold is not None:\n",
    "        cells_filter = np.logical_and(cells_filter, pidf['reads'] < duplets_threshold)\n",
    "        \n",
    "    cells = set(pidf.loc[cells_filter]['barcode'])\n",
    "    idfcells = idf.loc[[c in cells for c in idf['barcode']]]\n",
    "    print(f'Estimated number of cells: {len(cells)}')\n",
    "    plt.title('Cells calling')\n",
    "    plt.plot(np.log10(df_cells['n']), \n",
    "             np.log10(df_cells['count']), \n",
    "             label='Cell', linewidth=3, color='green')\n",
    "\n",
    "    plt.xlabel('Log10 number of barcodes')\n",
    "    plt.ylabel('Log10 fragments overlapping Peaks')\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    sns.distplot(np.log10(df_cells['count']))\n",
    "    plt.title('Cells UMI summary coverage distribution')\n",
    "    plt.xlabel('Log10 coverage')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    return idfcells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with PdfPages(os.path.join(figures_path, 'cell_calling.pdf')) as pdf:\n",
    "    idfcells = filter_cells(idf, noise_threshold=NOISE_THRESHOLD, duplets_threshold=DUPLETS_THRESHOLD)\n",
    "    pdf.savefig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup memory\n",
    "del idf\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peak-Barcode Matrix\n",
    "\n",
    "**Pipeline**\n",
    "Compute number of different UMIs which intersect with peak. Multiple copies of UMI are ignored.\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Cell Ranger ATAC-Seq**\n",
    "We produce a count matrix consisting of the counts of fragment ends (or cut sites) within each peak region for each barcode. This is the raw peak-barcode matrix and it captures the enrichment of open chromatin per barcode. The matrix is then filtered to consist of only cell barcodes, which is then used in subsequent analysis such as dimensionality reduction, clustering and visualization.\n",
    "\n",
    "A barcoded fragment may get sequenced multiple times due to PCR amplification. We mark duplicates in order to identify the original fragments that constitute the library and contribute to its complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T15:20:39.832474Z",
     "start_time": "2019-04-25T15:18:42.780874Z"
    }
   },
   "outputs": [],
   "source": [
    "def cellsdf_to_coverage(idfcells):\n",
    "    print('Barcode vs summary fragments overlap with peaks')\n",
    "    # 10x Genomics Cell Ranger ATAC-Seq marks duplicates\n",
    "    # 'count' ignores multiple reads per barcode at same position\n",
    "    cell_peak_cov_df = pd.pivot_table(idfcells, values='reads', \n",
    "                            index=['peak_chr', 'peak_start', 'peak_end', 'barcode'], \n",
    "                            aggfunc='count').reset_index()\n",
    "\n",
    "    cell_peak_cov_df['peak'] = cell_peak_cov_df['peak_chr'] + ':' + \\\n",
    "        cell_peak_cov_df['peak_start'].astype(str) + '-' + cell_peak_cov_df['peak_end'].astype(str)\n",
    "    cell_peak_cov_df.drop(columns=['peak_chr', 'peak_start', 'peak_end'], inplace=True)\n",
    "\n",
    "    print('Transforming dataframe to peaks x barcode format')\n",
    "    coveragedf = pd.pivot_table(cell_peak_cov_df, index='peak', columns='barcode', values='reads').fillna(0)\n",
    "    # Remove extra labels from pivot_table columns\n",
    "    coveragedf.columns = coveragedf.columns.values\n",
    "    coveragedf.index.name = None\n",
    "    return coveragedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_mtx_path = os.path.join(OUTPUT_DIR, 'cache', 'coverage.mtx')\n",
    "barcodes_path = os.path.join(OUTPUT_DIR, 'cache', 'barcodes.txt')\n",
    "peaks_path = os.path.join(OUTPUT_DIR, 'cache', 'peaks.txt')\n",
    "\n",
    "if os.path.exists(cov_mtx_path):\n",
    "    print(f'Loading coverage matrix {cov_mtx_path}')\n",
    "    csc = io.mmread(cov_mtx_path)\n",
    "    barcodes = [l.rstrip('\\n') for l in open(barcodes_path)]\n",
    "    peaks = [l.rstrip('\\n') for l in open(peaks_path)]\n",
    "    print(f'Loading into dataframe')\n",
    "    coveragedf = pd.DataFrame(csc.toarray(), index=peaks, columns=barcodes)\n",
    "    coveragedf.fillna(0, inplace=True)\n",
    "    # Cleanup memory\n",
    "    del csc\n",
    "    del barcodes\n",
    "    del peaks\n",
    "    gc.collect()    \n",
    "else:\n",
    "    print(f'No coverage matrix found {cov_mtx_path}')\n",
    "    coveragedf = cellsdf_to_coverage(idfcells)\n",
    "    print(f'Saving coverage matrix')\n",
    "    csc = sparse.csc_matrix(coveragedf.values, dtype=int)\n",
    "    io.mmwrite(cov_mtx_path, csc)\n",
    "\n",
    "    print('Saving barcodes')\n",
    "    with open(barcodes_path, 'w') as f:\n",
    "        for b in coveragedf.columns.values:\n",
    "            f.write(b + '\\n')\n",
    "\n",
    "    print('Saving peaks')\n",
    "    with open(peaks_path, 'w') as f:\n",
    "        for p in coveragedf.index.values:\n",
    "            f.write(p + '\\n')    \n",
    "    # Cleanup memory\n",
    "    del csc\n",
    "    gc.collect()    \n",
    "\n",
    "print('DF', coveragedf.shape[0], 'peaks x', coveragedf.shape[1], 'cells')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup memory\n",
    "del idfcells\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverage dataframe analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Peaks per cell with non-zero coverage filtration')\n",
    "peaks_per_cell = np.count_nonzero(coveragedf, axis=0)\n",
    "with PdfPages(os.path.join(figures_path, 'peaks_per_cell.pdf')) as pdf:\n",
    "    sns.distplot(np.log10(peaks_per_cell))\n",
    "    plt.title('Peaks with coverage > 0 per cell')\n",
    "    plt.xlabel('Log10 peaks')\n",
    "    plt.ylabel('Frequency')\n",
    "    pdf.savefig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with PdfPages(os.path.join(figures_path, 'umis.pdf')) as pdf:\n",
    "    print('Summary UMI distribution per factor')\n",
    "    factors = [FACTOR_FUNCTION(bc) for bc in coveragedf.columns]\n",
    "    for k, v in tqdm(FACTORS_MAP.items()):\n",
    "        sns.kdeplot(np.log10(coveragedf[coveragedf.columns[np.flatnonzero(np.equal(factors, k))]].sum()), \n",
    "                    label=v)\n",
    "    plt.title('UMI summary coverage distribution')\n",
    "    plt.xlabel('Log10 coverage')\n",
    "    plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cells per peak with non-zero coverage distribution')\n",
    "cells_per_peak = np.count_nonzero(coveragedf, axis=1)\n",
    "with PdfPages(os.path.join(figures_path, 'cells_per_peaks.pdf')) as pdf:\n",
    "    sns.distplot(np.log10(cells_per_peak))\n",
    "    plt.title('Cells with coverage > 0 per peak')\n",
    "    plt.xlabel('Log10 cells')\n",
    "    plt.ylabel('Frequency')\n",
    "    pdf.savefig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot peak coverage vs peak length\n",
    "with PdfPages(os.path.join(figures_path, 'peaks_coverage_vs_length.pdf')) as pdf:\n",
    "    sns.jointplot(x=np.log10(cells_per_peak), \n",
    "                  y=[int(re.split(':|-', p)[2]) - int(re.split(':|-', p)[1]) for p in coveragedf.index], \n",
    "                  alpha=0.1)\n",
    "    plt.xlabel('Log10 cells')\n",
    "    plt.ylabel('Peaks length')\n",
    "    plt.suptitle('Peak coverage vs peak length')\n",
    "    pdf.savefig()\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "\n",
    "**Cell Ranger** uses normalization to median coverage depth in each UMI for PCA analysis,\\\n",
    "and doesn't do any normalization for LSA (default scheme) - we **do** the same here.\n",
    "\n",
    "**Seurat** we employ a global-scaling normalization method “LogNormalize” that normalizes the feature expression measurements for each cell by the total expression, multiplies this by a scale factor (10,000 by default), and log-transforms the result.\n",
    "\n",
    "**SnapATAC** does not require population-level peak annotation, and instead\n",
    "assembles chromatin landscapes by directly clustering cells based on the similarity of\n",
    "their genome-wide accessibility profile. Using a regression-based normalization\n",
    "procedure, SnapATAC adjusts for differing read depth between cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection (Identification of highly variable features)\n",
    "\n",
    "**Pipeline**\n",
    "\n",
    "* mean >99% as alignment errors or housekeeping genes\n",
    "* std <1% as noise\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Cell Ranger ATAC** removed features selection with zero variance.\n",
    "\n",
    "**SnapATAC**\n",
    "The vast majority of the items in the cell-by-bin count matrix is “0”, some items have abnormally high coverage (often > 200) perhaps due to alignment error. Bins of exceedingly high coverage which likely represent the genomic regions that are invariable between cells such as housekeeping gene promoters were removed. We noticed that filtering bins of extremely low coverage perhaps due to random noise can also improve the robustness of the downstream clustering analysis. \n",
    "* Calculated the coverage of each bin using the binary matrix and \n",
    "* Removed the top **0.1%** items of the highest coverage\n",
    "* Normalized the coverage by log10(count + 1). \n",
    "* Log-scaled coverage obey approximately a gaussian distribution which is then converted into zscore. \n",
    "* Bins with zscore beyond ±2 were filtered before further analysis.\n",
    "\n",
    "**Signac**\n",
    "The largely binary nature of scATAC-seq data makes it challenging to perform ‘variable’ feature selection, as we do for scRNA-seq. Instead, we can choose to use only the top n% of features (peaks) for dimensional reduction, or remove features present in less that n cells with the FindTopFeatures function. Here, we will all features, though we note that we see very similar results when using only a subset of features (try setting min.cutoff to ‘q75’ to use the top 25% all peaks), with faster runtimes. Features used for dimensional reduction are automatically set as VariableFeatures for the Seurat object by this function.\n",
    "See for explanation: https://satijalab.org/signac/articles/pbmc_vignette.html\n",
    "\n",
    "\n",
    "\n",
    "**Seurat** calculate a subset of features that exhibit high cell-to-cell variation in the dataset (i.e, they are highly expressed in some cells, and lowly expressed in others). We and others have found that focusing on these genes in downstream analysis helps to highlight biological signal in single-cell datasets.\n",
    "\n",
    "<img src='assets/4.png' width=20%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Computing mean and std values for peaks...')\n",
    "peaksdf = pd.DataFrame({'mean': np.mean(coveragedf.values, axis=1), \n",
    "                        'std': np.std(coveragedf.values, axis=1)},\n",
    "                       index=coveragedf.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with PdfPages(os.path.join(figures_path, 'feature_selection_before.pdf')) as pdf:\n",
    "    # df is (peaks x barcodes)\n",
    "    sns.jointplot(x='mean', y='std', data=peaksdf, alpha=0.05)\n",
    "    pdf.savefig()\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(peaksdf, stdp=1, meanp=99):\n",
    "    print('Total peaks', len(peaksdf))\n",
    "    print('Feature selection: filter out most highly covered peaks as alignment errors or housekeeping genes.')\n",
    "    means_high = np.percentile(peaksdf['mean'], meanp)\n",
    "    mean_filter = peaksdf['mean'] < means_high\n",
    "    print('Feature selection: filter out non-variable peaks.')          \n",
    "    stds_low = np.percentile(peaksdf['std'], stdp)\n",
    "    std_filter = peaksdf['std'] > stds_low\n",
    "    peaks_filter = np.logical_and(mean_filter, std_filter)\n",
    "    print('Filtered peaks', sum(peaks_filter))\n",
    "    return peaks_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "peaks_filter = feature_selection(peaksdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with PdfPages(os.path.join(figures_path, 'feature_selection_after.pdf')) as pdf:\n",
    "    sns.jointplot(x='mean', y='std', data=pd.DataFrame({'mean': peaksdf.loc[peaks_filter]['mean'], \n",
    "                                                        'std': peaksdf.loc[peaks_filter]['std']}),\n",
    "                  alpha=0.05)\n",
    "    pdf.savefig()\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Check for markers')\n",
    "for k,v in marker_regions_peaks.items():\n",
    "    if v not in coveragedf.index:\n",
    "        print(f'{k} {v} is missing in coveragedf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction, Clustering and visualization\n",
    "\n",
    "**Pipeline** Use Cell Ranger ATAC approach and IRLB code for dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA (IDF + IRLBA + L2 normalization)\n",
    "\n",
    "**Cell Ranger ATAC** default method is **LSA**.\n",
    "\n",
    "1. IDF scaling\n",
    "2. IRLBA SVD decomposition to dim 15\n",
    "3. L2 normalization in low dimension projection\n",
    "4. Graph clustering\n",
    "5. T-SNE visualization\n",
    "\n",
    "**IRLBA**: The augmented implicitly restarted Lanczos bidiagonalization algorithm finds a few approximate largest singular values and corresponding singular vectors of a sparse or dense matrix using a method of Baglama and Reichel.\n",
    "\n",
    "PCA if your features are least sensitive (informative) towards the mean of the distribution, then it makes sense to subtract the mean. If the features are most sensitive towards the high values, then subtracting the mean does not make sense.\n",
    "\n",
    "SVD **does not subtract the means** but often as a first step projects the data on the mean of all data points. In this way the SVD first takes care of global structure - this is important for IDF transformation.\n",
    "\n",
    "**We found that the combination of these normalization techniques obviates the need to remove the first component.**\n",
    "The number of dimensions is fixed to 15 as it was found to sufficiently separate clusters visually and in a biologically meaningful way when tested on peripheral blood mononuclear cells (PBMCs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def lsa(df, dim):\n",
    "    print('Normalizing by IDF')\n",
    "    idf = np.log1p(df.shape[1]) - np.log1p(np.count_nonzero(df, axis=1))\n",
    "    df_idf = df.multiply(idf, axis=0).T # Transpose to (barcodes x peaks) format\n",
    "    del idf # Cleanup    \n",
    "    \n",
    "    print('Performing IRLBA without scaling or centering')\n",
    "    # Number of dimensions recommended by Cell Ranger ATAC-Seq algorithm\n",
    "    (_, _, v, _, _) = irlb(df_idf, dim)\n",
    "    # project the matrix to complete the transform: X --> X*v = u*d\n",
    "    df_idf_irlb = df_idf.dot(v) \n",
    "    assert df_idf_irlb.shape == (df_idf.shape[0], dim), 'check dimensions'\n",
    "    del df_idf # Cleanup\n",
    "\n",
    "    # IMPORTANT! We found that the combination of these normalization techniques \n",
    "    # obviates the need to remove the first component.\n",
    "    print('Normalization each barcode data point to unit L2-norm in the lower dimensional space')\n",
    "    idf_irlb_l2 = Normalizer(norm='l2').fit_transform(df_idf_irlb) # (n_samples, n_features) \n",
    "    return pd.DataFrame(idf_irlb_l2, index=df_idf_irlb.index, columns=df_idf_irlb.columns)\n",
    "\n",
    "# NO UMI normalization required for LSA approach!\n",
    "# IDF is invariant to scales, IRLBA don't need scaling.\n",
    "result_lsa = lsa(coveragedf.loc[peaks_filter], LSA_COMPONENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TSNE visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Processing t-SNE on L2-normalized data')\n",
    "tsne_coords = pd.DataFrame(TSNE(n_components=2).fit_transform(result_lsa.values),\n",
    "                           index=result_lsa.index,\n",
    "                           columns=['tsne1', 'tsne2'])\n",
    "print('t-SNE done!')\n",
    "# display(tsne_coords.head())\n",
    "\n",
    "print('Saving tSNE coordinates to tsne.tsv')\n",
    "tsne_coords.to_csv(os.path.join(OUTPUT_DIR, 'tsne.tsv'), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('tSNE on LSA normalized vs log10 coverate depth')\n",
    "with PdfPages(os.path.join(figures_path, 'tsne.pdf')) as pdf:\n",
    "    plot_colored(tsne_coords.values, np.log10(coveragedf.sum()), size=20)\n",
    "    pdf.savefig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional dupletes filtration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_feature_selection_dim_reduction(coveragedf):\n",
    "    # Feature selection\n",
    "    peaksdf = pd.DataFrame({'mean': np.mean(coveragedf.values, axis=1), \n",
    "                            'std': np.std(coveragedf.values, axis=1)},\n",
    "                            index=coveragedf.index)\n",
    "    peaks_filter = feature_selection(peaksdf)\n",
    "    # Normalization and dimensionality reduction\n",
    "    result_lsa = lsa(coveragedf.loc[peaks_filter], LSA_COMPONENTS)\n",
    "    return result_lsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reprocess coveragedf removing dupletes\n",
    "# Be careful: it can remove all the markers at all!\n",
    "# print('Clip dupletes by cutoff')\n",
    "# coveragedf = coveragedf.iloc[np.flatnonzero(np.log10(coveragedf.sum()) <= 3.8), :]\n",
    "\n",
    "# print('Normalization and dimensionality reduction')\n",
    "# result_lsa = process_feature_selection_dim_reduction(coveragedf)\n",
    "\n",
    "# print('Processing t-SNE on L2-normalized data')\n",
    "# tsne_coords = pd.DataFrame(TSNE(n_components=2).fit_transform(result_lsa.values),\n",
    "#                            index=result_lsa.index,\n",
    "#                            columns=['tsne1', 'tsne2'])\n",
    "# plot_colored(tsne_coords.values, np.log10(coveragedf.sum()), size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('tSNE on LSA normalized vs', FACTOR, FACTORS_MAP)\n",
    "factors = np.array([FACTOR_FUNCTION(bc) for bc in result_lsa.index])\n",
    "with PdfPages(os.path.join(figures_path, f'tsne_{FACTOR}.pdf')) as pdf:\n",
    "    plot_colored(tsne_coords.values, factors, clusters=True, size=20, alpha=0.1)\n",
    "    pdf.savefig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Uncomment to plot split by factor\n",
    "# print(FACTORS_MAP)\n",
    "# with PdfPages(os.path.join(figures_path, f'tsne_{FACTOR}_split.pdf')) as pdf:\n",
    "#     plot_colored_split(tsne_coords.values, factors, alpha=0.1)\n",
    "#     pdf.savefig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph clustering of LSA normalized data\n",
    "\n",
    "Clustering should:\n",
    "* produce clusters enough clusters 10-15 recommended by Cell Ranger\n",
    "* too small clusters are likely artifacts < 100 cells\n",
    "* centers of clusters should be inside the cluster, otherwise we should increase number of clusters OR increase lower dimension space size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_clustering(result, n_clusters):\n",
    "    # Ward linkage method as default with euclidean distance\n",
    "    print('Clustering', n_clusters)\n",
    "    clusters = AgglomerativeClustering(n_clusters=n_clusters).fit_predict(result).astype(int)\n",
    "\n",
    "    # reorder clusters by size descending\n",
    "    cluster_counter = Counter()\n",
    "    for c in clusters:\n",
    "        cluster_counter[c] += 1\n",
    "    clusters_reord = np.zeros(len(clusters), dtype=int)    \n",
    "    for i, (c, n) in enumerate(cluster_counter.most_common()):\n",
    "        clusters_reord[clusters == c] = i\n",
    "\n",
    "    return clusters_reord\n",
    "\n",
    "n_clusters = N_CLUSTERS\n",
    "clusters = graph_clustering(result_lsa, n_clusters)\n",
    "print(f'Saving clusters to clusters{n_clusters}.tsv')\n",
    "clusters_df = pd.DataFrame({'cluster': clusters}, index=result_lsa.index)\n",
    "clusters_df.to_csv(os.path.join(OUTPUT_DIR, f'clusters{n_clusters}.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "def clusters_summary(coveragedf, clusters):\n",
    "    # Clusters summary by factors\n",
    "    t = pd.DataFrame({FACTOR: [FACTOR_FUNCTION(bc) for bc in coveragedf.columns],\n",
    "                     'cluster': clusters, 'counts': np.ones(len(coveragedf.columns))}, \n",
    "                     index=coveragedf.columns)\n",
    "    clusters_sizes(t, 'counts')\n",
    "    clusters_factors(t, 'counts', FACTORS_MAP)    \n",
    "\n",
    "def clusters_hierarchy(X, clusters):\n",
    "    # Hierarchical clustering of X - mean normalized LSA coordinates per cluster\n",
    "    t = X.reset_index(drop=True).values\n",
    "    clusters_means = {\n",
    "        str(c): np.mean(t[np.flatnonzero(clusters == c), :], axis=0) for c in sorted(set(clusters))\n",
    "    }\n",
    "    clusters_means_df = pd.DataFrame(clusters_means).reset_index()\n",
    "    clusters_linkage = hierarchy.linkage(\n",
    "        clusters_means_df[[str(i) for i in set(clusters)]].T.values, method='ward'\n",
    "    )\n",
    "    hierarchy.dendrogram(clusters_linkage, orientation='top')\n",
    "    hierarchy.set_link_color_palette(None)  # reset to default after use\n",
    "    return clusters_linkage\n",
    "    \n",
    "def clusters_silhouette(X, clusters):\n",
    "    # X - mean normalized LSA coordinates per cluster\n",
    "    # The silhouette_score gives the average value for all the samples\n",
    "    silhouette_avg = silhouette_score(X, clusters)\n",
    "    print(\"For n_clusters =\", len(set(clusters)), \"the average silhouette_score is \", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, clusters)\n",
    "\n",
    "    plt.figure(figsize=(6, 10))\n",
    "    ax = plt.gca()\n",
    "\n",
    "    y_lower = 10\n",
    "    for i, c in enumerate(sorted(set(clusters))):\n",
    "        # Aggregate the silhouette scores for samples belongiterto cluster c, and sort them\n",
    "        cluster_silhouette_values = sample_silhouette_values[clusters == c]\n",
    "        cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster = cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax.text(-0.05, y_lower + 0.5 * size_cluster, str(c))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax.set_yticks([])  # Clear the yaxis labels / ticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Computing summary for clusters by', FACTOR)\n",
    "print(FACTORS_MAP)\n",
    "\n",
    "t = pd.DataFrame({FACTOR: [FACTOR_FUNCTION(bc) for bc in coveragedf.columns],\n",
    "                 'cluster': clusters, 'counts': np.ones(len(coveragedf.columns))}, \n",
    "                 index=coveragedf.columns)\n",
    "display(t[[FACTOR, 'counts']].groupby([FACTOR]).sum())\n",
    "\n",
    "with PdfPages(os.path.join(figures_path, 'clusters_sizes.pdf')) as pdf:\n",
    "    clusters_sizes(t, 'counts')\n",
    "    pdf.savefig()\n",
    "\n",
    "with PdfPages(os.path.join(figures_path, f'clusters_{FACTOR}.pdf')) as pdf:\n",
    "    clusters_factors(t, 'counts', FACTORS_MAP)\n",
    "    pdf.savefig() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with PdfPages(os.path.join(figures_path, 'clusters_umi.pdf')) as pdf:\n",
    "    ts = []\n",
    "    plt.figure(figsize=(int(n_clusters * 0.75), 6))\n",
    "    for c in tqdm(set(clusters)):\n",
    "        t = pd.DataFrame({'umi': np.log10(coveragedf.T.iloc[np.flatnonzero(clusters == c)].sum(axis=1) + 1)})\n",
    "        t['cluster'] = str(c)\n",
    "        ts.append(t)\n",
    "    sns.violinplot(data=pd.concat(ts), x='cluster', y='umi', order=[str(c) for c in sorted(set(clusters))])\n",
    "    plt.title('UMI summary coverage per cell')\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel('Summary log10 coverage')\n",
    "    plt.legend()\n",
    "    pdf.savefig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with PdfPages(os.path.join(figures_path, 'clusters_cell_peaks.pdf')) as pdf:\n",
    "    ts = []\n",
    "    plt.figure(figsize=(int(n_clusters * 0.75), 6))    \n",
    "    for c in tqdm(set(clusters)):\n",
    "        t = pd.DataFrame({\n",
    "            'nzpeaks': np.log10(np.count_nonzero(coveragedf.T.iloc[np.flatnonzero(clusters == c)], axis=1) + 1)\n",
    "        })\n",
    "        t['cluster'] = str(c)\n",
    "        ts.append(t)\n",
    "    sns.violinplot(data=pd.concat(ts), x='cluster', y='nzpeaks', order=[str(c) for c in sorted(set(clusters))])\n",
    "    plt.title('Peaks with non-zero coverage distribution per cell')\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    pdf.savefig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clusters visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_linkage = clusters_hierarchy(result_lsa, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouetter for clusters\n",
    "clusters_silhouette(result_lsa, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('tSNE on LSA normalized vs graph clusters')\n",
    "with PdfPages(os.path.join(figures_path, 'tsne_clusters.pdf')) as pdf:\n",
    "    plot_colored(tsne_coords.values, clusters, clusters=True, show_centers=True, size=20, alpha=0.5)\n",
    "    pdf.savefig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Uncomment to plot split by cluster\n",
    "# with PdfPages(os.path.join(figures_path, 'tsne_clusters_split.pdf')) as pdf:\n",
    "#     plot_colored_split(tsne_coords.values, clusters, size=5)\n",
    "#     pdf.savefig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse markers of regions vs clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Hierarchical clustering of mean coverage per peak in cluster')\n",
    "# Transpose to (barcodes x peaks) format\n",
    "t = coveragedf.T \n",
    "# Per cluster mean values for peaks\n",
    "clusters_means = {str(c): t.loc[clusters == c].mean() for c in sorted(set(clusters))}\n",
    "clusters_means_df = pd.DataFrame(clusters_means).reset_index()\n",
    "\n",
    "clusters_linkage_covdf = hierarchy.linkage(\n",
    "    clusters_means_df[[str(i) for i in set(clusters)]].T.values, method='ward'\n",
    ")\n",
    "hierarchy.dendrogram(clusters_linkage_covdf, orientation='top')\n",
    "hierarchy.set_link_color_palette(None)  # reset to default after use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Hierarchical clustering of coverage in markers')\n",
    "t = clusters_means_df[[str(c) for c in set(clusters)]].copy()\n",
    "t.index = clusters_means_df['index']\n",
    "mt = [(k, v) for k, v in marker_regions_peaks.items() if v in t.index]\n",
    "t = t.loc[[v for _, v in mt]]\n",
    "t.index = [k for k, _ in mt]\n",
    "# zscore per row\n",
    "# t = pd.DataFrame(zscore(t, axis=1), index=t.index, columns=t.columns)\n",
    "# 0-1 scale per row\n",
    "t = t.divide(t.max(axis=1), axis='rows')\n",
    "sns.clustermap(t, figsize=(1 + int(n_clusters / 3), 1 + int(len(mt) / 4)), cmap='bwr',\n",
    "              col_linkage=clusters_linkage)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regions_plot(regions, df, xs, ys, xlabel, ylabel, factors, clusters, binary):\n",
    "    print(f'Coverage z-score, distribution and fraction on non-zero covered peaks w.r.t {FACTOR} and clusters')\n",
    "    fregions = []\n",
    "    for k, peak in regions.items():\n",
    "        if peak in df.index:\n",
    "            fregions.append((k, peak))\n",
    "        else:\n",
    "            print(f'{k} {peak} is missing')\n",
    "            \n",
    "    fig = plt.figure(figsize=(4*4, 4*len(fregions)))\n",
    "    grid = plt.GridSpec(nrows=len(fregions), ncols=8, wspace=0.5, hspace=0.5, figure=fig)\n",
    "    for i, (k, peak) in enumerate(fregions):\n",
    "        if peak not in df.index:\n",
    "            print(f'{k} {peak} is missing')\n",
    "            continue\n",
    "        vals = df.loc[peak]\n",
    "        print(k, peak, 'cov>0:', sum(vals > 0), 'of', len(vals), 'max cov:', max(vals))\n",
    "\n",
    "        ax = plt.subplot(grid[i, 0:2])\n",
    "        # Plot zscores scatter plot with 2 segments colormap blue-white-red\n",
    "        zs = zscore(vals)\n",
    "        cmap = LinearSegmentedColormap.from_list(name='zscore', \n",
    "            colors=[(0, 'darkblue'), ((0 - np.min(zs)) / (np.max(zs) - np.min(zs)), 'white'), (1, 'red')])\n",
    "        plt.setp(ax, xticks=[], yticks=[])\n",
    "        ax.scatter(xs, ys, marker='.', c=zs, cmap=cmap, s=2, alpha=0.2)\n",
    "        plt.title(k)\n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(ylabel)        \n",
    "        \n",
    "        t = pd.DataFrame({'cluster': clusters, 'factor': factors, \n",
    "                          'val': [min(v, 1) for v in vals] if binary else vals})\n",
    "        \n",
    "        \n",
    "        ax = plt.subplot(grid[i, 2])\n",
    "        # mean non-zero covered peaks by type\n",
    "        tf = pd.pivot_table(t, values='val', index=['factor'], aggfunc='mean').reset_index()\n",
    "        sns.barplot(x='factor', y='val', data=tf)\n",
    "        ax.tick_params(axis='x', labelrotation=90)\n",
    "        plt.ylabel('')\n",
    "\n",
    "        ax = plt.subplot(grid[i, 3:])\n",
    "        # mean non-zero covered peaks by cluster and factor\n",
    "        tc = t.groupby(['cluster', 'factor']).mean().reset_index()\n",
    "        sns.barplot(x='cluster', y='val', data=tc, hue='factor')\n",
    "        plt.ylabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# regions_plot(marker_regions_peaks, coveragedf, \n",
    "#              tsne_coords['tsne1'], tsne_coords['tsne2'],\n",
    "#              'tsne1', 'tsne2',\n",
    "#             [FACTORS_MAP[f] for f in factors], clusters, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteractive clustering refinement\n",
    "\n",
    "* Using clustering on LSA normalized data refine clusters\n",
    "* Create BAM files for each group\n",
    "* Call peaks on each cluster\n",
    "* Process clustering iteration\n",
    "* Merge clustering into a final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fragments(fragments_tsv_gz):\n",
    "    print('Loading full fragments dataframe')\n",
    "    return pd.read_csv(fragments_tsv_gz, sep='\\t', \n",
    "                       names=['chr', 'start', 'end', 'barcode', 'reads'], compression='gzip')\n",
    "\n",
    "def get_fragments_and_clusters(fragments_tsv_gz, barcodes, clusters):\n",
    "    fdf = get_fragments(fragments_tsv_gz)\n",
    "\n",
    "    print('Aggregating fragments and clusters')\n",
    "    t = pd.merge(left=fdf, \n",
    "                 right=pd.DataFrame({'barcode': barcodes, 'cluster': clusters}), \n",
    "                 left_on='barcode', right_on='barcode')\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = get_fragments_and_clusters(FRAGMENTS_FILE_10X, clusters_df.index, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refinement_groups = [(1, 2, 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_bigwig(df, bw_path, step):\n",
    "    chr_lengths = df[['chr', 'end']].groupby('chr').max().reset_index()\n",
    "    with pyBigWig.open(bw_path, 'w') as bw:\n",
    "        bw.addHeader(list(zip(chr_lengths['chr'], chr_lengths['end'])))\n",
    "        for chromosome in tqdm(chr_lengths['chr']):\n",
    "            df_chr = df.loc[df['chr'] == chromosome].sort_values(['start', 'end'])\n",
    "            starts = list(df_chr['start'])\n",
    "            ends = list(df_chr['end'])\n",
    "            reads = list(df_chr['reads'])\n",
    "            chr_length = int(chr_lengths.loc[chr_lengths['chr'] == chromosome]['end'])\n",
    "            values = np.zeros(int(math.floor(chr_length / step)) + 1)\n",
    "\n",
    "            for i in range(len(df_chr)):\n",
    "                # Ignore PCR duplicates here!\n",
    "                # v = reads[i] \n",
    "                v = 1.0\n",
    "                si = int(math.floor(starts[i] / step)) \n",
    "                ei = int(math.floor(ends[i] / step)) \n",
    "                if ei == si:\n",
    "                    values[si] += v\n",
    "                else:\n",
    "                    values[si] += v / 2 \n",
    "                    values[ei] += v / 2\n",
    "\n",
    "            non_zero = values > 0\n",
    "            non_zero_inds = np.flatnonzero(non_zero)\n",
    "            starts_np = non_zero_inds * step\n",
    "            ends_np = (non_zero_inds + 1) * step\n",
    "            values_np = values[non_zero]\n",
    "            # RPM normalization\n",
    "            values_np = values_np * (1_000_000.0 / sum(values_np))\n",
    "            chroms_np = np.array([chromosome] * sum(non_zero))\n",
    "            bw.addEntries(chroms_np, starts=starts_np, ends=ends_np, values=values_np)                     \n",
    "\n",
    "def write_cluster_bigwigs(fragments_tsv_gz, barcodes, clusters, bigwigs_path, step):\n",
    "    df_for_bigwigs = get_fragments_and_clusters(fragments_tsv_gz, barcodes, clusters)\n",
    "\n",
    "    for cluster in sorted(set(clusters)):\n",
    "        bw_path = os.path.join(bigwigs_path, f'cluster_{cluster}.bw')\n",
    "        print('Processing', cluster, bw_path)\n",
    "        df_for_bigwigs_cluster = df_for_bigwigs.loc[df_for_bigwigs['cluster'] == cluster]\n",
    "        write_bigwig(df_for_bigwigs_cluster, bw_path, step)\n",
    "\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time \n",
    "import pysam\n",
    "import os\n",
    "\n",
    "beds_path = os.path.join(OUTPUT_DIR, 'cache', '1')\n",
    "! mkdir {beds_path}\n",
    "\n",
    "for refined_group in refinement_groups:\n",
    "    rgn = '_'.join(map(lambda c: str(c), refined_group))\n",
    "    bed_path = os.path.join(beds_path, f'cluster_{rgn}.bed')\n",
    "    fragments_path = os.path.join(beds_path, f'cluster_{rgn}.tsv.gz')\n",
    "    bw_path = os.path.join(beds_path, f'cluster_{rgn}.bw')\n",
    "    print('Processing', refined_group)\n",
    "    tg = t.loc[t['cluster'].isin(refined_group)]\n",
    "    print('Saving reads', bed_path)\n",
    "    tg[['chr', 'start', 'end']].to_csv(bed_path, sep='\\t', header=False, index=False)\n",
    "    print('Saving fragments', fragments_path)\n",
    "    tg[['chr', 'start', 'end', 'barcode', 'reads']].to_csv(fragments_path, sep='\\t', \n",
    "                                                           header=False, index=False,\n",
    "                                                           compression='gzip')\n",
    "    print('Saving visualization', bw_path)\n",
    "    write_bigwig(tg, bw_path, step=200)\n",
    "    del tg\n",
    "    \n",
    "\n",
    "# # variable to hold barcode index\n",
    "# CB_hold = 'unset'\n",
    "# itr = 0\n",
    "# # read in upsplit file and process reads line by line\n",
    "# samfile = pysam.AlignmentFile(unsplit_file, \"rb\")\n",
    "\n",
    "# start_time = time.time()\n",
    "# for read in samfile.fetch(until_eof=True):\n",
    "#     if(len([x for x in read.tags if x[0] == \"CB\"])>0):\n",
    "#         # barcode itr for current read\n",
    "#         CB_itr = read.get_tag( 'CB')\n",
    "#         if(CB_itr in df_barcodes[0].tolist()):\n",
    "#             # if change in barcode or first line; open new file  \n",
    "#             if(CB_itr!=CB_hold or itr==0):\n",
    "#                 # close previous split file, only if not first read in file\n",
    "#                 if(itr!=0):\n",
    "#                     split_file.close()\n",
    "#                 CB_hold = CB_itr\n",
    "#                 itr+=1\n",
    "#                 split_file = pysam.AlignmentFile(out_dir + \"{}.bam\".format(CB_hold), \"wb\", template=samfile)\n",
    "#             # write reads with the same barcode to file\n",
    "#             split_file.write(read)    \n",
    "# split_file.close()\n",
    "# samfile.close()\n",
    "# end_time = time.time()\n",
    "\n",
    "\n",
    "# print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process cluster fragments.tsv.gz and clusters peaks, long operation!\n",
    "idf = intersect_fragments_and_peaks(\n",
    "    os.path.join(beds_path, 'cluster_1_2_3.tsv.gz'),\n",
    "    os.path.join(beds_path, 'cluster_1_2_3_peaks.narrowPeak', \n",
    "    BLACKLIST_FILE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_coverage(idf):\n",
    "    idfcells = filter_cells(idf, noise_threshold=NOISE_THRESHOLD, duplets_threshold=DUPLETS_THRESHOLD)\n",
    "    # Compute coverage df\n",
    "    coveragedf = cellsdf_to_coverage(idfcells)\n",
    "coveragedf = process_coverage(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup memory\n",
    "del idf\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_lsa = process_feature_selection_dim_reduction(coveragedf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factors_summary(coveragedf):\n",
    "    t = pd.DataFrame({FACTOR: [FACTOR_FUNCTION(bc) for bc in coveragedf.columns],\n",
    "                     'counts': np.ones(len(coveragedf.columns))}, \n",
    "                     index=coveragedf.columns)\n",
    "    display(t[[FACTOR, 'counts']].groupby([FACTOR]).sum())\n",
    "    \n",
    "factors_summary(coveragedf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Processing t-SNE on L2-normalized data')\n",
    "tsne_coords = pd.DataFrame(TSNE(n_components=2).fit_transform(result_lsa.values),\n",
    "                           index=result_lsa.index,\n",
    "                           columns=['tsne1', 'tsne2'])\n",
    "plot_colored(tsne_coords.values, np.log10(coveragedf.sum()), size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reprocess coveragedf removing dupletes\n",
    "\n",
    "print('Clip dupletes by cutoff')\n",
    "coveragedf = coveragedf.iloc[np.flatnonzero(np.log10(coveragedf.sum()) <= 4.1), :]\n",
    "\n",
    "print('Normalization and dimensionality reduction')\n",
    "result_lsa = process_feature_selection_dim_reduction(coveragedf)\n",
    "\n",
    "print('Processing t-SNE on L2-normalized data')\n",
    "tsne_coords = pd.DataFrame(TSNE(n_components=2).fit_transform(result_lsa.values),\n",
    "                           index=result_lsa.index,\n",
    "                           columns=['tsne1', 'tsne2'])\n",
    "plot_colored(tsne_coords.values, np.log10(coveragedf.sum()), size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for n_clusters in [5, 10, 20]:\n",
    "    clusters = graph_clustering(result_lsa, n_clusters) \n",
    "    clusters_summary(coveragedf, clusters)\n",
    "    plt.show()\n",
    "    clusters_hierarchy(result_lsa, clusters)\n",
    "    plt.show()\n",
    "    clusters_silhouette(result_lsa, clusters)\n",
    "    plot_colored(tsne_coords.values, clusters, clusters=True, show_centers=True, size=20, alpha=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create bigwig per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf = get_fragments(os.path.join(beds_path, 'cluster_1_2_3.tsv.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for n_clusters in [5, 10, 20]:\n",
    "    print('Processing', n_clusters)\n",
    "    group_bigwigs_path = os.path.join(OUTPUT_DIR, f'cache/1/cluster_1_2_3/{n_clusters}')\n",
    "    if os.path.exists(group_bigwigs_path):\n",
    "        continue  # Already processed\n",
    "    ! mkdir {group_bigwigs_path}\n",
    "    clusters = graph_clustering(result_lsa, n_clusters)\n",
    "    print('Aggregating fragments and clusters')\n",
    "    df_for_bigwigs = pd.merge(left=fdf, \n",
    "                              right=pd.DataFrame({'barcode': result_lsa.index, 'cluster': clusters}), \n",
    "                              left_on='barcode', right_on='barcode')\n",
    "    for cluster in sorted(set(clusters)):\n",
    "        bw_path = os.path.join(group_bigwigs_path, f'cluster_{cluster}.bw')\n",
    "        print('Processing', cluster, bw_path)\n",
    "        write_bigwig(df_for_bigwigs.loc[df_for_bigwigs['cluster'] == cluster], bw_path, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "del fdf\n",
    "del df_for_bigwigs\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusters visualization vs 10x Cell Ranger ATAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print('These clusters into Cell Ranger ATAC coordinates') \n",
    "# mergeddf1 = pd.merge(pd.DataFrame({'Barcode': tsne_coords.index, 'Cluster': clusters}),\n",
    "#                      right=tsne10xdf, \n",
    "#                      left_on='Barcode', right_on='Barcode')\n",
    "\n",
    "# mergeddf2 = pd.merge(pd.DataFrame({'Barcode': tsne_coords.index,\n",
    "#                                   'tsne1': tsne_coords['tsne1'],\n",
    "#                                   'tsne2': tsne_coords['tsne2']}),\n",
    "#                      right=clusters10xdf, \n",
    "#                      left_on='Barcode', right_on='Barcode')\n",
    "\n",
    "\n",
    "# fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(18, 7))\n",
    "\n",
    "# ax1.title.set_text('Clusters in 10x tsne coordinates')\n",
    "# plot_colored(mergeddf1[['TSNE-1', 'TSNE-2']].values, \n",
    "#              mergeddf1['Cluster'], \n",
    "#              clusters=True,\n",
    "#              show_centers=True,\n",
    "#              size=20, alpha=0.3, fig_ax=(fig, ax1))\n",
    "\n",
    "# ax2.title.set_text('10x clusters in tsne coordinates')\n",
    "# plot_colored(mergeddf2[['tsne1', 'tsne2']].values, \n",
    "#              mergeddf2['Cluster'], \n",
    "#              clusters=True,\n",
    "#              show_centers=True,\n",
    "#              size=20, alpha=0.3, fig_ax=(fig, ax2))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP visualization\n",
    "umap_coords = pd.DataFrame(umap.UMAP().fit_transform(result_lsa.values)[:, :2], \n",
    "                           index=result_lsa.index,\n",
    "                           columns=['umap1', 'umap2'])\n",
    "\n",
    "print('Saving UMAP coordinates to umap.tsv')\n",
    "umap_coords.to_csv(os.path.join(OUTPUT_DIR, 'umap.tsv'), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('UMAP on LSA normalized vs log10 coverate depth')\n",
    "with PdfPages(os.path.join(figures_path, 'umap.pdf')) as pdf:\n",
    "    plot_colored(umap_coords.values, np.log10(coveragedf.sum()), size=20)\n",
    "    pdf.savefig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('tSNE on LSA normalized vs', FACTOR, FACTORS_MAP)\n",
    "with PdfPages(os.path.join(figures_path, f'umap_{FACTOR}.pdf')) as pdf:\n",
    "    factors = [FACTOR_FUNCTION(bc) for bc in result_lsa.index]\n",
    "    plot_colored(umap_coords.values, factors, clusters=True, size=20, alpha=0.01)\n",
    "    pdf.savefig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Uncomment to plot split by factor\n",
    "# print(FACTORS_MAP)\n",
    "# with PdfPages(os.path.join(figures_path, f'umap_{FACTOR}_spit.pdf')) as pdf:\n",
    "#     plot_colored_split(umap_coords.values, factors, size=5, alpha=0.1)\n",
    "#     pdf.savefig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('UMAP on LSA normalized vs clusters')\n",
    "with PdfPages(os.path.join(figures_path, 'umap_clusters.pdf')) as pdf:\n",
    "    plot_colored(umap_coords.values, clusters, clusters=True, show_centers=True, size=20, alpha=0.3)\n",
    "    pdf.savefig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Uncomment to plot split by cluster\n",
    "# print('UMAP on LSA normalized vs graph clusters')\n",
    "# with PdfPages(os.path.join(figures_path, 'umap_clusters_split.pdf')) as pdf:\n",
    "#     plot_colored_split(umap_coords.values, clusters, size=5)\n",
    "#     pdf.savefig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize markers of interest vs clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Analysing REGIONS of interest TSNE')\n",
    "with PdfPages(os.path.join(figures_path, 'tsne_markers.pdf')) as pdf:\n",
    "    regions_plot(marker_regions_peaks, coveragedf, \n",
    "                 tsne_coords['tsne1'], tsne_coords['tsne2'],\n",
    "                 'tsne1', 'tsne2',\n",
    "                [FACTORS_MAP[f] for f in factors], clusters, True)\n",
    "    pdf.savefig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Analysing REGIONS of interest UMAP')\n",
    "with PdfPages(os.path.join(figures_path, 'umap_markers.pdf')) as pdf:\n",
    "    regions_plot(marker_regions_peaks, coveragedf, \n",
    "                 umap_coords['umap1'], umap_coords['umap2'],\n",
    "                 'umap1', 'umap2',\n",
    "                 [FACTORS_MAP[f] for f in factors], clusters, True)\n",
    "    pdf.savefig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BigWig RPM normalized profiles\n",
    "\n",
    "**Pipeline** Split UMI reads by clusters and use RPKM normalization.\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Seurat** We created pseudo-bulk ATAC-seq profiles by pooling together cells with for each cell type. Each cell type showed enriched accessibility near canonical marker genes. Chromatin accessibility tracks are normalized to sequencing depth (RPKM normalization) in each pooled group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bigwigs_path = os.path.join(OUTPUT_DIR, 'bigwig')\n",
    "! mkdir -p {bigwigs_path}\n",
    "\n",
    "write_cluster_bigwigs(FRAGMENTS_FILE_10X, clusters_df.index, clusters, bigwigs_path, step=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save cluster mean values to table with closest genes information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_genes = locations_closest_genes(list(coveragedf.index), transcripts_tss)\n",
    "# Restore 'peak' to be able to merge on it\n",
    "closest_genes['peak'] = closest_genes['chr'] +\\\n",
    "                        ':' + closest_genes['start'].astype(str) +\\\n",
    "                        '-' + closest_genes['end'].astype(str)\n",
    "# display(closest_genes.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_means_genes_df = pd.concat([closest_genes.reset_index(), clusters_means_df], axis=1).drop(columns=['index'])\n",
    "# Save peak without dashes or colons\n",
    "clusters_means_genes_df['peak'] = [re.sub('[:-]', '_', p) for p in clusters_means_genes_df['peak']]\n",
    "\n",
    "print('Saving clusters peaks_means to clusters_peaks_values.tsv')\n",
    "# display(clusters_means_genes_df.head())\n",
    "clusters_means_genes_df.to_csv(os.path.join(OUTPUT_DIR, 'clusters_peaks_values.tsv'), sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differential peaks analysis (Markers)\n",
    "\n",
    "**Pipeline**\n",
    "\n",
    "Use Cell Ranger ATAC to get differential markers. \n",
    "\n",
    "<hr>\n",
    "\n",
    "**Cell Ranger ATAC**\n",
    "For each peak we perform test cluster vs others, so that several clusters may yield significant results vs others, we take into account only the cluster with greatest mean value.\\\n",
    "Test is is perfromed using Cell Ranger default testing procedure based on Negative Binomial GLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch 10x Genomics Cell Ranger DE\n",
    "def run_10x_differential_expression(df, clusters, fdr=0.05):\n",
    "    \"\"\" Compute differential expression for each cluster vs all other cells\n",
    "        Args: df          - feature expression data (peak x barcode)\n",
    "              clusters    - 0-based cluster labels\n",
    "              fdr         - false discovery rate\"\"\"\n",
    "\n",
    "    print(\"Computing params...\")\n",
    "    m = df.values\n",
    "    sseq_params = compute_sseq_params(m)\n",
    "    peaks = list(df.index)    \n",
    "\n",
    "    cluster_markers = []\n",
    "    for cluster in tqdm(sorted(set(clusters))):\n",
    "        in_cluster = clusters == cluster\n",
    "        group_a = np.flatnonzero(in_cluster)\n",
    "        group_b = np.flatnonzero(np.logical_not(in_cluster))\n",
    "        print(f'Computing DE for cluster {cluster}...')\n",
    "\n",
    "        de_result = sseq_differential_expression(m, group_a, group_b, sseq_params)\n",
    "        de_result['cluster'] = [cluster] * len(de_result)\n",
    "        de_result['peak'] = peaks\n",
    "        passed_fdr = de_result['adjusted_p_value'] < fdr\n",
    "        passed_log2_fc = de_result['log2_fold_change'] > 0\n",
    "        # Filter out different by statistical tests with log2 fold change > 0\n",
    "        passed = np.logical_and(passed_fdr, passed_log2_fc)\n",
    "        print('DE:', sum(passed), 'of', len(de_result), \n",
    "              'FDR:', sum(passed_fdr), 'log2fc>0:', sum(passed_log2_fc))\n",
    "\n",
    "        passed_de = de_result.loc[passed]\n",
    "        cluster_de_markers = passed_de[\n",
    "            ['cluster', 'peak', 'norm_mean_a', 'norm_mean_b', 'log2_fold_change', 'p_value', 'adjusted_p_value']\n",
    "        ].rename(columns={'norm_mean_a': 'norm_mean_cluster', 'norm_mean_b': 'norm_mean_others'}) \n",
    "\n",
    "        cluster_markers.append(cluster_de_markers)    \n",
    "\n",
    "    return pd.concat(cluster_markers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell Ranger ATAC like differential analysis\n",
    "# Each peak is tested independently, no normalization to peak length required.\n",
    "diff_markers = run_10x_differential_expression(coveragedf.loc[peaks_filter], clusters)\n",
    "print('Total diff markers', len(diff_markers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markers summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of differential markers per cluster')\n",
    "t = diff_markers[['cluster']].copy()\n",
    "t['count'] = 1\n",
    "display(pd.pivot_table(t, values='count', index=['cluster'], aggfunc=np.sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotate markers with genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markers_with_genes = pd.merge(left=diff_markers[['cluster', 'norm_mean_cluster', 'norm_mean_others', \n",
    "                                                 'log2_fold_change', 'p_value', 'adjusted_p_value', 'peak']],\n",
    "                         right=closest_genes,\n",
    "                         left_on='peak',\n",
    "                         right_on='peak')\n",
    "\n",
    "# Save peak without dashes or colons\n",
    "t = markers_with_genes['peak'].copy()\n",
    "markers_with_genes['peak'] = [re.sub('[:-]', '_', p) for p in markers_with_genes['peak']]\n",
    "\n",
    "# rearrange columns, sort\n",
    "markers_with_genes = markers_with_genes[['chr', 'start', 'end', 'peak',\n",
    "                                         'cluster', 'norm_mean_cluster', 'norm_mean_others', \n",
    "                                         'log2_fold_change', 'p_value', 'adjusted_p_value', \n",
    "                                         'gene', 'distance']].sort_values(by=['cluster', 'adjusted_p_value'])\n",
    "# display(markers_with_genes.head())\n",
    "\n",
    "print('Saving all the markers', len(markers_with_genes), 'to markers.tsv')\n",
    "markers_with_genes.to_csv(os.path.join(OUTPUT_DIR, 'markers.tsv'), sep='\\t', index=None)\n",
    "\n",
    "# Restore peaks\n",
    "markers_with_genes['peak'] = t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing top markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_markers(df, n_clusters, top=100):\n",
    "    top_markers = pd.concat([\n",
    "        df.loc[df['cluster'] == c].sort_values(by=['adjusted_p_value']).head(top) \n",
    "                    for c in range(0, n_clusters)])\n",
    "    return top_markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Computing Z scores for mean values in cluster for top markers')\n",
    "top_markers = get_top_markers(markers_with_genes, 15, top=100)\n",
    "t = coveragedf.T # Transpose to (barcodes x peaks) format\n",
    "t = t[list(top_markers['peak'])] # top markers only\n",
    "\n",
    "t = pd.concat([pd.DataFrame(t.loc[clusters == cluster].mean()).T for cluster in set(clusters)])\n",
    "t.index = range(n_clusters)\n",
    "\n",
    "# Transform to Z-score\n",
    "for c in t.columns:\n",
    "    t[c] = zscore(t[c])\n",
    "\n",
    "print('Clustermap of Z scores')\n",
    "with PdfPages(os.path.join(figures_path, 'markers_z.pdf')) as pdf:\n",
    "    sns.clustermap(t.T, col_cluster=False, row_cluster=False, figsize=(10, 10), cmap=plt.cm.seismic)\n",
    "    pdf.savefig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top markers visualization on t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T15:37:58.681824Z",
     "start_time": "2019-04-25T15:37:54.162643Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def show_top_markers_z(t, xs, ys, markers_with_genes, clusters, n_clusters, top=100):\n",
    "    top_markers = get_top_markers(markers_with_genes, n_clusters, top)\n",
    "    nrows, ncols = n_rows_columns(n_clusters)\n",
    "    plt.subplots(nrows=nrows, ncols=ncols, figsize=(5 * ncols, 4 * nrows))\n",
    "    for i, cluster in enumerate(tqdm(set(clusters))):\n",
    "        # Table (marker for cluster x cells)\n",
    "        markers_df = t.loc[top_markers.loc[top_markers['cluster'] == cluster]['peak']].T.copy()\n",
    "\n",
    "        # Z scale for each marker peak across all cells\n",
    "        for c in markers_df.columns:\n",
    "            markers_df[c] = zscore(markers_df[c])\n",
    "\n",
    "        # Average Z score for each cell\n",
    "        zs = markers_df.T.mean()\n",
    "        ax = plt.subplot(nrows, ncols, i + 1)\n",
    "        # Average is much more stable than coverage, we can use linear colormap here\n",
    "        plt.setp(ax, xticks=[], yticks=[])    \n",
    "        # Plot zscores scatter plot with 2 segments colormap blue-white-red\n",
    "        cmap = LinearSegmentedColormap.from_list(name='zscore', \n",
    "            colors=[(0, 'darkblue'), ((0 - np.min(zs)) / (np.max(zs) - np.min(zs)), 'white'), (1, 'red')])        \n",
    "        sc = ax.scatter(xs, ys, s=5, c=zs, cmap=cmap, edgecolor='none', alpha=1)\n",
    "#         sc = ax.scatter(xs, ys, s=5, c=zs, cmap=plt.cm.seismic, edgecolor='none', alpha=1)\n",
    "        plt.colorbar(sc)\n",
    "        plt.title(f'Cluster {cluster}')\n",
    "#             plt.xlabel('tSNE axis 1')\n",
    "#             plt.ylabel('tSNE axis 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('t-SNE based Z-SCORE visualizations for markers, mean z-score for 100 top markers')\n",
    "    \n",
    "with PdfPages(os.path.join(figures_path, f'tsne_diff_markers_100.pdf')) as pdf:    \n",
    "    show_top_markers_z(coveragedf, tsne_coords['tsne1'], tsne_coords['tsne2'], \n",
    "                       markers_with_genes, clusters, n_clusters, 100)\n",
    "    pdf.savefig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('t-SNE based Z-SCORE visualizations for markers, z-score for 1 top marker')\n",
    "t = coveragedf\n",
    "top_markers_1 = get_top_markers(markers_with_genes, n_clusters, top=1)\n",
    "top_markers_map = {}\n",
    "for i, row in top_markers_1.iterrows():\n",
    "    top_markers_map[str(row['cluster'])] = row['peak']\n",
    "    \n",
    "with PdfPages(os.path.join(figures_path, 'tsne_diff_markers_1.pdf')) as pdf:\n",
    "    regions_plot(top_markers_map, coveragedf, \n",
    "                 tsne_coords['tsne1'], tsne_coords['tsne2'],\n",
    "                 'tsne1', 'tsne2',\n",
    "                 factors, clusters, True)\n",
    "    pdf.savefig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save differential markers to BED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T15:39:39.518557Z",
     "start_time": "2019-04-25T15:39:38.869915Z"
    }
   },
   "outputs": [],
   "source": [
    "cluster_peaks_path = os.path.join(OUTPUT_DIR, 'markers')\n",
    "! mkdir -p {cluster_peaks_path}\n",
    "print('Cleanup peaks_clusters', cluster_peaks_path)\n",
    "for f in glob.glob(f'{cluster_peaks_path}/markers_*.bed'):\n",
    "    os.remove(f)\n",
    "\n",
    "mlap_max = (-np.log10(diff_markers.loc[diff_markers['adjusted_p_value'] != 0]['adjusted_p_value'])).max()\n",
    "print('BED scoring as -log10 adjusted pval')\n",
    "print('Max of -log10 adjusted pval', mlap_max)\n",
    "\n",
    "for c in set(clusters):\n",
    "    bed_file = os.path.join(cluster_peaks_path, f'markers_{c}.bed')\n",
    "    markers_cluster = diff_markers.loc[diff_markers['cluster'] == c].sort_values(\n",
    "        ['log2_fold_change'], ascending=False\n",
    "    ).iloc[::-1]\n",
    "    markers_cluster.index = range(len(markers_cluster))\n",
    "    print('Saving cluster', c, 'marker peaks to', bed_file)\n",
    "    with open(bed_file, 'w') as bed:\n",
    "        for i in range(len(markers_cluster)):\n",
    "            peak = markers_cluster['peak'][i]\n",
    "            ap = markers_cluster['adjusted_p_value'][i]\n",
    "            mlap = -np.log10(ap) if ap != 0.0 else mlap_max\n",
    "            line = '{}\\t{}\\t{}\\t.\\n'.format(\n",
    "                re.sub(':|-', '\\t', peak), \n",
    "                c,\n",
    "                mlap)\n",
    "            bed.write(line)\n",
    "print('Done')                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation data for single cell explorer\n",
    "\n",
    "Preprocess data for single cell explorer https://ctlab.github.io/SCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_cse(df, clusters_df, tsne_coords, umap_coords, markers,\n",
    "                  sce_path, token, name, public, organism,\n",
    "                  markers_x_column='gene', markers_gene_column='peak'):\n",
    "    print('SCE path', sce_path)\n",
    "    print('SCE token', token)\n",
    "    print('SCE name', name)\n",
    "    print('SCE public', public)\n",
    "    print('SCE organism', organism)\n",
    "    \n",
    "    # Sanity check indexes\n",
    "    assert np.array_equal(tsne_coords.index, umap_coords.index)\n",
    "    assert np.array_equal(tsne_coords.index, df.columns)\n",
    "    assert np.array_equal(tsne_coords.index, df.columns)\n",
    "    assert np.array_equal(tsne_coords.index, df.columns)\n",
    "    assert np.array_equal(clusters_df.index, df.columns)\n",
    "    clusters = clusters_df['cluster']\n",
    "\n",
    "    print('Processing', 'dataset.json')\n",
    "    with open(os.path.join(sce_path, 'dataset.json'), 'w') as f:\n",
    "        f.write(json.dumps({\n",
    "            'token': token,\n",
    "            'public': public,\n",
    "            'name': name,\n",
    "            'organism': organism,\n",
    "            'cells': df.shape[1]\n",
    "        }))\n",
    "\n",
    "\n",
    "    print('Processing', 'plot_data.json')\n",
    "    fields = {\n",
    "      \"tSNE_1\": {\n",
    "        \"type\": \"numeric\",\n",
    "        \"range\": [\n",
    "          min(tsne_coords.loc[df.columns.values]['tsne1']),\n",
    "          max(tsne_coords.loc[df.columns.values]['tsne1'])\n",
    "        ]\n",
    "      },\n",
    "      \"tSNE_2\": {\n",
    "        \"type\": \"numeric\",\n",
    "        \"range\": [\n",
    "          min(tsne_coords.loc[df.columns.values]['tsne2']),\n",
    "          max(tsne_coords.loc[df.columns.values]['tsne2'])\n",
    "        ]\n",
    "      },\n",
    "      \"UMAP_1\": {\n",
    "        \"type\": \"numeric\",\n",
    "        \"range\": [\n",
    "          min(umap_coords.loc[df.columns.values]['umap1']),\n",
    "          max(umap_coords.loc[df.columns.values]['umap1'])\n",
    "        ]\n",
    "      },\n",
    "      \"UMAP_2\": {\n",
    "        \"type\": \"numeric\",\n",
    "        \"range\": [\n",
    "          min(umap_coords.loc[df.columns.values]['umap2']),\n",
    "          max(umap_coords.loc[df.columns.values]['umap2'])\n",
    "        ]\n",
    "      },\n",
    "      \"Cluster\": {\n",
    "        \"type\": \"factor\",\n",
    "        \"levels\": [str(c) for c in set(clusters)]\n",
    "      },\n",
    "      \"Age\": {\n",
    "        \"type\": \"factor\",\n",
    "        \"levels\": list(FACTORS_MAP.values())\n",
    "      },    \n",
    "      \"nUmi\": {\n",
    "        \"type\": \"numeric\",\n",
    "        \"range\": [\n",
    "          min(df.sum()),\n",
    "          max(df.sum()),\n",
    "        ]\n",
    "      }\n",
    "    }\n",
    "\n",
    "    data = pd.DataFrame({      \n",
    "        \"tSNE_1\": tsne_coords.loc[df.columns.values]['tsne1'].tolist(),\n",
    "        \"tSNE_2\": tsne_coords.loc[df.columns.values]['tsne2'].tolist(),\n",
    "        \"UMAP_1\": umap_coords.loc[df.columns.values]['umap1'].tolist(),\n",
    "        \"UMAP_2\": umap_coords.loc[df.columns.values]['umap2'].tolist(),\n",
    "        \"Cluster\": [str(c) for c in clusters_df.loc[df.columns.values]['cluster']],\n",
    "        \"Age\": [FACTORS_MAP[FACTOR_FUNCTION(bc)] for bc in df.columns],\n",
    "        \"nUmi\": df.sum().tolist(),\n",
    "        \"_row\": list(df.columns)\n",
    "    }).to_dict('records')\n",
    "\n",
    "    annotations = {\n",
    "      \"tsne_Cluster_centers\": {\n",
    "        \"type\": \"text\",\n",
    "        \"value\": \"Cluster\",\n",
    "        \"coords\": [\n",
    "          \"tSNE_1\",\n",
    "          \"tSNE_2\"\n",
    "        ],\n",
    "        \"data\": [\n",
    "          {\n",
    "            \"Cluster\": str(c),\n",
    "            \"tSNE_1\": float(np.mean(tsne_coords.loc[df.columns.values].values[\n",
    "                np.flatnonzero(clusters_df.loc[df.columns.values]['cluster'] == c), 0])),\n",
    "            \"tSNE_2\": float(np.mean(tsne_coords.loc[df.columns.values].values[\n",
    "                np.flatnonzero(clusters_df.loc[df.columns.values]['cluster'] == c), 1])),\n",
    "            \"Text\": str(c)\n",
    "          } for c in set(clusters)\n",
    "        ]    \n",
    "      },\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(sce_path, 'plot_data.json'), 'w') as f:\n",
    "        f.write(json.dumps({\n",
    "            \"fields\": fields,\n",
    "            \"data\": data,\n",
    "            \"annotations\": annotations,\n",
    "        }))\n",
    "\n",
    "\n",
    "    # This file simply contains gene names, cell barcodes/names and total UMI per cell. \n",
    "    # This file must reflect row and column names of matrix data.h5 which contains expression data.    \n",
    "    print('Processing', 'exp_data.json')\n",
    "    with open(os.path.join(sce_path, 'exp_data.json'), 'w') as f:\n",
    "        f.write(json.dumps({\n",
    "          \"genes\": list(df.index.values),\n",
    "          \"barcodes\": list(df.columns.values),\n",
    "          \"totalCounts\": df.sum(axis=0).astype(int).tolist()\n",
    "        }))\n",
    "\n",
    "\n",
    "    print('Processing', 'data.h5')\n",
    "    ! rm {sce_path}/data.h5\n",
    "    # Fix missing locks\n",
    "    os.environ['HDF5_USE_FILE_LOCKING'] = 'FALSE'\n",
    "    # Don't use pandas to_hdf5 because of unused overhead.\n",
    "    # IMPORTANT: transpose matrix because of different order!\n",
    "    # Column-major order is used by Fortran, Matlab, R, and most underlying core linear algebra libraries (BLAS). \n",
    "    # Row-major ordering is sometimes called “C” style ordering and column-major ordering “Fortran” style. \n",
    "    # Python/NumPy refers to the orderings in array flags as C_CONTIGUOUS and F_CONTIGUOUS, respectively.\n",
    "    # https://cran.r-project.org/web/packages/reticulate/vignettes/arrays.html\n",
    "    expdf = df.astype(int).clip(upper=1) # Clip coverage to 0-1 for better visualization\n",
    "    with h5py.File(os.path.join(sce_path, 'data.h5'), 'w') as hf:\n",
    "        hf.create_dataset(\"expression/mat\", \n",
    "                          data=expdf.T.values, # Column-major ordering!\n",
    "                          compression=\"gzip\", compression_opts=9, \n",
    "                          dtype='i4') # int4 is required here for SCE, see https://github.com/ctlab/SCE/issues/4\n",
    "\n",
    "\n",
    "    print('Processing', 'markers.json')\n",
    "    # Take into account only 100 top markers\n",
    "    with open(os.path.join(sce_path, 'markers.json'), 'w') as f:\n",
    "        f.write(json.dumps({\n",
    "            \"Cluster\": [ \n",
    "              {\n",
    "                  'X': row[markers_x_column],\n",
    "                  'p_val': row['p_value'],\n",
    "                  'p_val_adj': row['adjusted_p_value'],\n",
    "                  'avg_logFC': row['log2_fold_change'],\n",
    "                  'pct.1': row['norm_mean_cluster'],\n",
    "                  'pct.2': row['norm_mean_others'],\n",
    "                  'cluster': str(row['cluster']),\n",
    "                  'gene': row[markers_gene_column],\n",
    "                  'distance': row['distance']\n",
    "              } for index, row in markers.iterrows()\n",
    "          ]\n",
    "        }))\n",
    "\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sce_path = os.path.join(OUTPUT_DIR, 'sce')\n",
    "! mkdir -p {sce_path}\n",
    "filtereddf = coveragedf.loc[peaks_filter]\n",
    "export_to_cse(filtereddf, clusters_df, tsne_coords, umap_coords, \n",
    "              markers=markers_with_genes,\n",
    "              sce_path=sce_path,\n",
    "              token=SCE_TOKEN, \n",
    "              name=SCE_NAME,\n",
    "              public=SCE_PUBLIC,\n",
    "              organism=SCE_ORGANISM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save single cell explorer data with respect to genes instead of peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Associated genes {len(closest_genes)}')\n",
    "t = closest_genes[['gene', 'peak']].copy()\n",
    "t['distance'] = np.abs(closest_genes['distance'])\n",
    "t = t.loc[t['distance'] < SCE_PEAK_GENE_TSS_MAX_DISTANCE]\n",
    "print(f'Associated genes with distance to TSS < {SCE_PEAK_GENE_TSS_MAX_DISTANCE} {len(t)}')\n",
    "\n",
    "print('Building mapping peak -> gene, closest peaks is mapped to gene, second closest with suffix _1, etc')\n",
    "t.sort_values(by=['gene', 'distance'], inplace=True)\n",
    "associated_genes_map = {}\n",
    "prev_gene = None\n",
    "prev_count = 1\n",
    "for _, (gene, peak, distance) in t.iterrows():\n",
    "    if prev_gene != gene:\n",
    "        associated_genes_map[peak] = gene\n",
    "        prev_count = 1\n",
    "        prev_gene = gene\n",
    "    else:\n",
    "        associated_genes_map[peak] = f'{gene}_{prev_count}'\n",
    "        prev_count += 1\n",
    "        \n",
    "print('Preparing coverage dataframe per gene')\n",
    "genes_filtereddf = filtereddf.loc[filtereddf.index.isin(associated_genes_map)].copy()\n",
    "genes_filtereddf.index = [associated_genes_map[peak] for peak in genes_filtereddf.index]\n",
    "\n",
    "print('Preparing top markers by gene')\n",
    "associated_markers_with_genes = \\\n",
    "    markers_with_genes.loc[np.abs(markers_with_genes['distance']) < SCE_PEAK_GENE_TSS_MAX_DISTANCE].copy()\n",
    "\n",
    "print('Updating markers genes names by distance to gene with prefix')\n",
    "associated_markers_with_genes['gene'] = [\n",
    "    associated_genes_map[f'{chr}:{start}-{end}'] for chr, start, end in \n",
    "        zip(associated_markers_with_genes['chr'],\n",
    "            associated_markers_with_genes['start'],\n",
    "            associated_markers_with_genes['end'])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes_sce_path = os.path.join(OUTPUT_DIR, 'sce_genes')\n",
    "! mkdir -p {genes_sce_path}\n",
    "\n",
    "export_to_cse(genes_filtereddf, clusters_df, tsne_coords, umap_coords, \n",
    "              markers=associated_markers_with_genes,\n",
    "              sce_path=genes_sce_path,\n",
    "              token=f'{SCE_TOKEN}_genes', \n",
    "              name=f'{SCE_NAME} by genes',\n",
    "              public=SCE_PUBLIC,\n",
    "              organism=SCE_ORGANISM,\n",
    "              markers_x_column='peak', markers_gene_column='gene')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "840px",
    "left": "0px",
    "right": "1228px",
    "top": "110px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
