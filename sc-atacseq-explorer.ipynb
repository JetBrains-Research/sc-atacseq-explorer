{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SC-ATACSEQ-EXPLORER PIPELINE\n",
    "\n",
    "This single cell ATAC-Seq analysis pipeline is designed for intergative analysis of\n",
    "dataset, initially processed by 10x Genomics Cell Ranger ATAC.\n",
    "\n",
    "**In addition** to 10x Genomics results it offers:\n",
    "\n",
    "* Flexible and clear data preprocessing and normalization methods\n",
    "* Summary on different conditions in case of aggregated dataset\n",
    "* Different types of clustering followed by heatmap and t-SNE visualizations in low dimensions space\n",
    "* Top cluster markers visualization on heatmap / t-SNE plot\n",
    "* Closest genes annotations for peaks and clusters\n",
    "* Annotated cell-specific genes analysis\n",
    "* BigWig and BED files for clusters and markers ready-to-be-visualized in [JBR Genome Browser](https://research.jetbrains.org/groups/biolabs/tools/jbr-genome-browser) by JetBrains Research\n",
    "* Data preparation for single cell explorer by [Artyomov Lab](http://artyomovlab.wustl.edu/site/), Washington University in St.Louis\n",
    "\n",
    "Required 10x Genomics Cell Cell Ranger ATAC files:\n",
    "\n",
    "* `fragments.tsv` - fragments matrix file provided by Cell Ranger\n",
    "* `peaks.bed` - peaks file (union of peaks) in case of merging\n",
    "* `clusters.csv` - Graph clustering after median normalization, IDF scaling, SVD projection and L2 normalization\n",
    "* `projection.csv` - T-SNE 2d project of all the cells\n",
    "\n",
    "Pipeline steps:\n",
    "\n",
    "* Cell Calling\n",
    "* Peak barcode matrix\n",
    "* UMI normalization\n",
    "* Feature selection\n",
    "* Dimensionality reduction\n",
    "* TSNE\n",
    "* Differential markers analysis\n",
    "* Supervised annotation of clusters by Gene markers\n",
    "\n",
    "Other pipelines:\n",
    "\n",
    "* **Cell Ranger ATAC-Seq** https://support.10xgenomics.com/single-cell-atac/software/pipelines/latest/algorithms/overview\n",
    "* **Seurat** https://www.biorxiv.org/content/biorxiv/early/2018/11/02/460147.full.pdf (not for sc-ATAC-Seq https://github.com/satijalab/seurat/issues/1422)\n",
    "* **Scasat** https://academic.oup.com/nar/article/47/2/e10/5134327\n",
    "* **SnapATAC** https://www.biorxiv.org/content/10.1101/615179v2\n",
    "* **Scater** https://academic.oup.com/bioinformatics/article/33/8/1179/2907823\n",
    "\n",
    "Questions and comments are welcome at oleg dot shpynov at jetbrains dot com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "FRAGMENTS_FILE = TODO\n",
    "# PEAKS_FILE = TODO\n",
    "PEAKS_FILE = TODO\n",
    "CLUSTERS_FILE = TODO\n",
    "TSNE_FILE = TODO\n",
    "\n",
    "# ! wget http://mitra.stanford.edu/kundaje/akundaje/release/blacklists/mm10-mouse/mm10.blacklist.bed.gz\n",
    "# ! guznip mm10.blacklist.bed.gz\n",
    "BLACKLIST_FILE = TODO\n",
    "# ! wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M22/gencode.vM22.annotation.gtf.gz \n",
    "# ! gunzip gencode.vM22.annotation.gtf.gz\n",
    "GTF_FILE = TODO\n",
    "\n",
    "# representative DNase hypersensitivity sites for mm10 https://www.encodeproject.org/annotations/ENCSR489XTV/\n",
    "DNASE_FILE = TODO\n",
    "\n",
    "FACTOR = TODO\n",
    "FACTORS_MAP = {1: 'f1', 2: 'f2'}\n",
    "FACTOR_FUNCTION = lambda x: 1 if x.endswith('-1') else 2\n",
    "\n",
    "OUTPUT_DIR = TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARKERS_REGIONS = {\n",
    "    'NCR1 promoter': 'chr7:4337400-4337800',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T15:15:44.892229Z",
     "start_time": "2019-04-25T15:15:38.151598Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format ='retina'\n",
    "\n",
    "import glob\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "from pybedtools import BedTool\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Fix random seed\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peaks file stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peaks_df = pd.read_csv(PEAKS_FILE, sep='\\t', header=None)\n",
    "print('Peaks', len(peaks_df))\n",
    "sns.distplot(peaks_df[2] - peaks_df[1])\n",
    "plt.title('Peak length distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overlap of peaks with representative DNAse hypersensitive sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnase = BedTool(DNASE_FILE)\n",
    "peaks_file = BedTool(PEAKS_FILE)\n",
    "peak_count = peaks_file.count()\n",
    "overlap = peaks_file.intersect(dnase, wa=True, u=True).count()\n",
    "peak_by_dnase = 0 if overlap == 0 else overlap * 100.0 / peak_count\n",
    "overlap = dnase.intersect(peaks_file, wa=True, u=True).count()\n",
    "dnase_by_peak = overlap * 100.0 / dnase.count()\n",
    "print('Fraction of peaks overlapping with representative DNAse', int(peak_by_dnase), '%')\n",
    "print('Fraction of representative DNAse overlapping with peaks', int(dnase_by_peak), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Cell Calling\n",
    "\n",
    "**Pipeline** uses **noise_threshold** and **duplets_threshold**.\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Cell Ranger ATAC** aggregation: when combining data from multiple GEM groups, the cellranger-atac aggr pipeline automatically equalizes the sensitivity of the groups before merging, which is the recommended approach in order to avoid the batch effect introduced by sequencing depth. **Default method**: Subsample fragments from higher-depth GEM wells until they all have an equal number of unique fragments per cell.\n",
    "\n",
    "For each barcode:\n",
    "* we have the record of mapped high-quality fragments that passed all filters (the fragments.tsv file).\n",
    "* Having determined peaks prior to this, we use the number of fragments that overlap any peak regions.\n",
    "* Separate the signal from noise.\n",
    "\n",
    "**This works better in practice as compared to naively using the number of fragments per barcode.** \n",
    "We first subtract a depth-dependent fixed count from all barcode counts to model whitelist contamination. This fixed count is the estimated number of fragments per barcode that originated from a different GEM, assuming a contamination rate of 0.02. Then we fit a mixture model of two negative binomial distributions to capture the signal and noise. Setting an odds ratio of 1000, we separate the barcodes that correspond to real cells from the non-cell barcodes.\n",
    "\n",
    "**Scasat** If a cell has open peaks below a user defined threshold (default: 50 peaks) in the peak accessibility matrix we would remove that cell from subsequent analysis. Also peaks not observed across a user defined number of valid cells (default: 10 cells) are not considered for further downstream analysis.\n",
    "\n",
    "**SnapATAC** alternative approach -  1kb, 5kb and 10kb resolution.\n",
    "\n",
    "Existing computational methods rely on **pre-defined** regions of transposase accessibility\n",
    "identified from the aggregate signals. ...\n",
    "\n",
    "**Limitations**: \n",
    "1. It requires sufficient number of single cell profiles to create robust aggregate signal for peak calling. \n",
    "2. The cell type identification is biased toward the most abundant cell types in the tissues. \n",
    "3. These techniques lack the ability to reveal regulatory elements in the rare cell populations which are underrepresented in the aggregate signal. This concern is critical, for example, in brain tissue, where key neuron types may represent less than 1% of all cells while still playing a critical role in the neural circuit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Fragments file provided by cell ranger\n",
    "fragments_file_gz = FRAGMENTS_FILE\n",
    "\n",
    "# BedTools doesn't work with gz file, so unzip it\n",
    "!gunzip {fragments_file_gz}\n",
    "fragments_file = re.sub('\\.gz', '', fragments_file_gz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intersect using bedtools rather than pybedtools, because they are too slow for files of this size!\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def intersect_fragments_and_peaks(fragments_file, peaks_file, blacklist_file):\n",
    "    print('Fragments')\n",
    "    ! wc -l {fragments_file}\n",
    "    idf = None\n",
    "    print('Blacklist')\n",
    "    ! wc -l {blacklist_file}\n",
    "    print('Peaks')\n",
    "    ! wc -l {peaks_file}\n",
    "\n",
    "    with tempfile.TemporaryDirectory(prefix='pipeline') as td:\n",
    "        print('Filtering out non-standard chromosomes')\n",
    "        chr_filtered_file = os.path.join(td, 'chromosome_filtered.bed')\n",
    "        ! cat {fragments_file} | grep -i -E 'chr[0-9mt]+[^_]' > {chr_filtered_file}\n",
    "        ! wc -l {chr_filtered_file}\n",
    "\n",
    "        print('Blacklist regions filtration')\n",
    "        blacklist_filtered_file = os.path.join(td, 'blacklist_filtered.bed')\n",
    "        ! bedtools intersect -v -a {chr_filtered_file} -b {blacklist_file} > {blacklist_filtered_file}\n",
    "        ! wc -l {blacklist_filtered_file}\n",
    "\n",
    "        print('Fragments and peaks intersection')\n",
    "        intersection_file = os.path.join(td, 'intersection.bed')\n",
    "        ! bedtools intersect -wa -wb -a {blacklist_filtered_file} -b {peaks_file} > {intersection_file}\n",
    "        ! wc -l {intersection_file}\n",
    "\n",
    "        idf = pd.read_csv(intersection_file, sep='\\t', header=None)\n",
    "        icolnames = ['chr', 'start', 'end', 'barcode', 'reads', 'peak_chr', 'peak_start', 'peak_end']\n",
    "        assert len(idf.columns) >= len(icolnames)\n",
    "        idf.rename(columns={o: n for (o, n) in zip(idf.columns[:len(icolnames)], icolnames)}, inplace=True)\n",
    "        idf = idf[icolnames] # Reorder and drop others\n",
    "    print('Done intersecting of fragments and peaks')\n",
    "    return idf\n",
    "\n",
    "idf = intersect_fragments_and_peaks(fragments_file, PEAKS_FILE, BLACKLIST_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_cells(idf, noise_threshold=None, duplets_threshold=None):\n",
    "    # 10x Genomics Cell Ranger ATAC-Seq marks duplicates\n",
    "    # 'count' ignores multiple reads per barcode at same position\n",
    "    pidf = pd.pivot_table(idf, values='reads', index=['barcode'], aggfunc='count')\n",
    "    pidf.reset_index(level=0, inplace=True)\n",
    "    \n",
    "    print('Total barcodes', len(pidf))    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    # n is for shoulder graph\n",
    "    counts = sorted(pidf['reads'], reverse=True)\n",
    "    df_counts = pd.DataFrame(data={'count': counts, 'n': range(1, len(counts) + 1)})\n",
    "    cells_filter = [True] * len(df_counts)\n",
    "    \n",
    "    if noise_threshold is not None:\n",
    "        noise_filter = df_counts['count'] <= noise_threshold\n",
    "        cells_filter = np.logical_and(cells_filter, np.logical_not(noise_filter))\n",
    "        df_noise = df_counts.loc[noise_filter]\n",
    "        print('Noise', len(df_noise))\n",
    "        plt.plot(np.log10(df_noise['n']), \n",
    "                 np.log10(df_noise['count']), \n",
    "                 label='Noise', linewidth=3, color='red')        \n",
    "\n",
    "    if duplets_threshold is not None:\n",
    "        duplets_filter = df_counts['count'] >= duplets_threshold\n",
    "        cells_filter = np.logical_and(cells_filter, np.logical_not(duplets_filter))\n",
    "        df_duplets = df_counts.loc[duplets_filter]\n",
    "        print('Duplets', len(df_duplets))\n",
    "        plt.plot(np.log10(df_duplets['n']), \n",
    "                 np.log10(df_duplets['count']), \n",
    "                 label='Duplets', linewidth=3, color='orange')        \n",
    "\n",
    "    df_cells = df_counts.loc[cells_filter]\n",
    "\n",
    "    cells_filter = [True] * len(pidf)\n",
    "    if noise_threshold is not None:\n",
    "        cells_filter = np.logical_and(cells_filter, pidf['reads'] > noise_threshold)\n",
    "    if duplets_threshold is not None:\n",
    "        cells_filter = np.logical_and(cells_filter, pidf['reads'] < duplets_threshold)\n",
    "        \n",
    "    cells = set(pidf.loc[cells_filter]['barcode'])\n",
    "    idfcells = idf.loc[[c in cells for c in idf['barcode']]]\n",
    "    print('Estimated number of cells', len(cells))\n",
    "\n",
    "    print('Preparing UMI shoulder graph')\n",
    "    plt.plot(np.log10(df_cells['n']), \n",
    "             np.log10(df_cells['count']), \n",
    "             label='Cell', linewidth=3, color='green')\n",
    "\n",
    "    plt.xlabel('Log10 number of barcodes')\n",
    "    plt.ylabel('Log10 fragments overlapping Peaks')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "    print('Cells UMI distribution')\n",
    "    sns.distplot(np.log10(df_cells['count']))\n",
    "    plt.title('UMI summary coverage distribution')\n",
    "    plt.xlabel('Log10 summary coverage')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "    return idfcells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfcells = filter_cells(idf, noise_threshold=200, duplets_threshold=8000)\n",
    "# Previous version of analysis\n",
    "# idfcells = filter_cells(idf, noise_threshold=150, duplets_threshold=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peak-Barcode Matrix\n",
    "\n",
    "**Pipeline**\n",
    "Compute number of different UMIs which intersect with peak. Multiple copies of UMI are ignored.\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Cell Ranger ATAC-Seq**\n",
    "We produce a count matrix consisting of the counts of fragment ends (or cut sites) within each peak region for each barcode. This is the raw peak-barcode matrix and it captures the enrichment of open chromatin per barcode. The matrix is then filtered to consist of only cell barcodes, which is then used in subsequent analysis such as dimensionality reduction, clustering and visualization.\n",
    "\n",
    "A barcoded fragment may get sequenced multiple times due to PCR amplification. We mark duplicates in order to identify the original fragments that constitute the library and contribute to its complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T15:20:39.832474Z",
     "start_time": "2019-04-25T15:18:42.780874Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Barcode vs summary fragments overlap with peaks')\n",
    "# 10x Genomics Cell Ranger ATAC-Seq marks duplicates\n",
    "# 'count' ignores multiple reads per barcode at same position\n",
    "pdf = pd.pivot_table(idfcells, values='reads', \n",
    "                     index=['peak_chr', 'peak_start', 'peak_end', 'barcode'], \n",
    "                     aggfunc='count').reset_index()\n",
    "\n",
    "pdf['peak'] = pdf['peak_chr'] + ':' + pdf['peak_start'].astype(str) + '-' + pdf['peak_end'].astype(str)\n",
    "pdf.drop(columns=['peak_chr', 'peak_start', 'peak_end'], inplace=True)\n",
    "# display(pdf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T15:21:52.504624Z",
     "start_time": "2019-04-25T15:20:54.469583Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Transforming dataframe to peaks x barcode format')\n",
    "fulldf = pd.pivot_table(pdf, index='peak', columns='barcode', values='reads').fillna(0)\n",
    "# Remove extra labels from pivot_table columns\n",
    "fulldf.columns = fulldf.columns.values\n",
    "fulldf.index.name = None\n",
    "# display(fulldf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Summary UMI distribution per factor')\n",
    "factors = [FACTOR_FUNCTION(bc) for bc in fulldf.columns]\n",
    "for k, v in tqdm(FACTORS_MAP.items()):\n",
    "    sns.kdeplot(np.log10(fulldf[fulldf.columns[np.flatnonzero(np.equal(factors, k))]].sum()), label=v)\n",
    "plt.title('UMI summary coverage distribution')\n",
    "plt.xlabel('Summary coverage')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "\n",
    "**Pipeline** \n",
    "* Global scaling to median coverage of UMI for coverage reporting\n",
    "* Peak length normalization for cluster analysis\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Cell Ranger** uses normalization to median coverage depth in each UMI - we do the same here.\n",
    "\n",
    "**Seurat** we employ a global-scaling normalization method “LogNormalize” that normalizes the feature expression measurements for each cell by the total expression, multiplies this by a scale factor (10,000 by default), and log-transforms the result.\n",
    "\n",
    "**SnapATAC** does not require population-level peak annotation, and instead\n",
    "assembles chromatin landscapes by directly clustering cells based on the similarity of\n",
    "their genome-wide accessibility profile. Using a regression-based normalization\n",
    "procedure, SnapATAC adjusts for differing read depth between cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffexp import estimate_size_factors\n",
    "\n",
    "print('UMI normalization by median fragments count per barcode (RPM)') \n",
    "# (peaks x barcodes) format\n",
    "size_factors = estimate_size_factors(fulldf.values)\n",
    "normdf = fulldf / size_factors # per column, i.e. barcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Peak length normalization')\n",
    "lengths = [int(re.split(':|-', p)[2]) - int(re.split(':|-', p)[1]) for p in fulldf.index]\n",
    "rpkdf = fulldf.divide(lengths, axis=0) * 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection (Identification of highly variable features)\n",
    "\n",
    "**Pipeline**\n",
    "\n",
    "* mean >99% as alignment errors or housekeeping genes\n",
    "* std <1% as noise\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Cell Ranger ATAC** removed features selection with zero variance.\n",
    "\n",
    "**SnapATAC**\n",
    "The vast majority of the items in the cell-by-bin count matrix is “0”, some items have abnormally high coverage (often > 200) perhaps due to alignment error. Bins of exceedingly high coverage which likely represent the genomic regions that are invariable between cells such as housekeeping gene promoters were removed. We noticed that filtering bins of extremely low coverage perhaps due to random noise can also improve the robustness of the downstream clustering analysis. \n",
    "* Calculated the coverage of each bin using the binary matrix and \n",
    "* Removed the top **0.1%** items of the highest coverage\n",
    "* Normalized the coverage by log10(count + 1). \n",
    "* Log-scaled coverage obey approximately a gaussian distribution which is then converted into zscore. \n",
    "* Bins with zscore beyond ±2 were filtered before further analysis.\n",
    "\n",
    "**Seurat** calculate a subset of features that exhibit high cell-to-cell variation in the dataset (i.e, they are highly expressed in some cells, and lowly expressed in others). We and others have found that focusing on these genes in downstream analysis helps to highlight biological signal in single-cell datasets.\n",
    "\n",
    "Seurat feature selection visualization\n",
    "<img src='assets/4.png' width=20%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(df, stdp=1, meanp=99):\n",
    "    # df is (peaks x barcodes)\n",
    "    print('Computing Mean vs SD of coverage')\n",
    "    means = df.T.mean()\n",
    "    stds = df.T.std()\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    sns.jointplot(x=means, y=stds)\n",
    "    plt.title('Mean vs SD')\n",
    "    plt.show()\n",
    "\n",
    "    # Filter out most highly covered peaks as alignment errors or housekeeping genes. \n",
    "    means_high = np.percentile(means, meanp)\n",
    "    peaks_filter = means < means_high\n",
    "    \n",
    "    # Filter out non-variable peaks\n",
    "    stds_low = np.percentile(stds, stdp)\n",
    "    peaks_filter = np.logical_and(stds_low < stds, means < means_high)\n",
    "    peaks_filter_indexes = np.flatnonzero(peaks_filter)\n",
    "            \n",
    "    print('After feature selection')\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    sns.jointplot(x=means[peaks_filter_indexes], y=stds[peaks_filter_indexes])\n",
    "    plt.title('Mean vs SD')\n",
    "    plt.show()\n",
    "\n",
    "    return peaks_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Normalization to peak length to take into account signal strength, not total coverage\n",
    "peaks_filter = feature_selection(rpkdf, meanp=99)\n",
    "# Previous analysis\n",
    "# peaks_filter = feature_selection(fulldf, meanp=99)\n",
    "print('Total peaks', len(rpkdf))\n",
    "print('Filtered peaks', sum(peaks_filter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Peaks with non-zero coverage distribution')\n",
    "sns.distplot(np.log10(np.count_nonzero(fulldf.loc[peaks_filter], axis=1)))\n",
    "plt.title('Peaks per cell')\n",
    "plt.xlabel('Log10 peaks')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Check marker regions of interest')\n",
    "peaks = fulldf.index[np.flatnonzero(peaks_filter)]\n",
    "marker_regions_peaks = {}\n",
    "for k, r in MARKERS_REGIONS.items():\n",
    "    cr,sr,er = re.split(':|-', r)\n",
    "    tp = [(c,s,e) for (c,s,e) in map(lambda p: re.split(':|-', p), peaks) \n",
    "        if c == cr and not (int(er) < int(s) or int(e) < int(sr))]\n",
    "    assert tp, f'{k} not found'\n",
    "    assert len(tp) == 1, f'Too many peaks for region {k}'\n",
    "    marker_regions_peaks[k] = c,s,e = tp[0]\n",
    "    print('Region', k, 'Peak', f'{c}:{s}-{e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction, Clustering and t-SNE Projection\n",
    "\n",
    "**Pipeline** Use Cell Ranger ATAC approach and IRLB code for dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA (median and log scaling + classic algorithm)\n",
    "1. UMI normalization\n",
    "2. Log scale\n",
    "3. IRLBA SVD decomposition to dim 10 instead of PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T15:30:46.979292Z",
     "start_time": "2019-04-25T15:30:46.949482Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler, MinMaxScaler\n",
    "\n",
    "print('PCA on features peaks vs log10 sum coverage')\n",
    "# Number of dimensions recommended by Cell Ranger ATAC-Seq algorithm\n",
    "pca = PCA(n_components=15)\n",
    "result_pca = pca.fit_transform(np.log1p(normdf.loc[peaks_filter].T)) # (n_samples x n_features)\n",
    "print('Explained variation', np.sum(pca.explained_variance_ratio_))\n",
    "plot_colored(result_pca, np.log10(normdf.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA (IDF + IRLBA + L2 normalization)\n",
    "\n",
    "**Cell Ranger ATAC** default method is **LSA**.\n",
    "\n",
    "1. IDF scaling\n",
    "2. IRLBA SVD decomposition to dim 15\n",
    "3. L2 normalization in low dimension projection\n",
    "4. Graph clustering\n",
    "5. T-SNE visualization\n",
    "\n",
    "**We found that the combination of these normalization techniques obviates the need to remove the first component.**\n",
    "The number of dimensions is fixed to 15 as it was found to sufficiently separate clusters visually and in a biologically meaningful way when tested on peripheral blood mononuclear cells (PBMCs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "from irlb import irlb\n",
    "\n",
    "def lsa(df, dim):\n",
    "    print('Normalizing by IDF')\n",
    "    idf = np.log1p(df.shape[1]) - np.log1p(np.count_nonzero(df, axis=1))\n",
    "    df_idf = df.multiply(idf, axis=0).T # Transpose to (barcodes x peaks) format\n",
    "    \n",
    "    print('Performing IRLBA without scaling or centering')\n",
    "    # Number of dimensions recommended by Cell Ranger ATAC-Seq algorithm\n",
    "    (_, _, v, _, _) = irlb(df_idf, dim)\n",
    "    # project the matrix to complete the transform: X --> X*v = u*d\n",
    "    df_idf_irlb = df_idf.dot(v) \n",
    "    assert df_idf_irlb.shape == (df_idf.shape[0], dim), 'check dimensions'\n",
    "\n",
    "    # IMPORTANT! We found that the combination of these normalization techniques \n",
    "    # obviates the need to remove the first component.\n",
    "    print('Normalization each barcode data point to unit L2-norm in the lower dimensional space')\n",
    "    idf_irlb_l2 = Normalizer(norm='l2').fit_transform(df_idf_irlb) # (n_samples, n_features) \n",
    "    return pd.DataFrame(idf_irlb_l2, index=df_idf_irlb.index, columns=df_idf_irlb.columns)\n",
    "\n",
    "# NO UMI normalization required for LSA approach!\n",
    "# IDF is invariant to scales, IRLBA don't need scaling.\n",
    "# Normalization to peak length to take into account signal strength, not total coverage\n",
    "result_lsa = lsa(rpkdf.loc[peaks_filter], 15)\n",
    "# Previous analysis\n",
    "# result_lsa = lsa(fulldf.loc[peaks_filter], 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP for dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP visualization\n",
    "import umap\n",
    "umap_coords = umap.UMAP().fit_transform(result_lsa.values)\n",
    "\n",
    "print('Saving UMAP coordinates to umap.tsv')\n",
    "pd.DataFrame(umap_coords[:2, :], columns=['umap1', 'umap2']).to_csv(OUTPUT_DIR + '/umap.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_colored(data, c,\n",
    "                 clusters=False, show_centers=False, \n",
    "                 size=10, alpha=0.5):\n",
    "    \"\"\" Plot colored scatterplot \"\"\"\n",
    "    cmap = plt.cm.get_cmap('jet', len(set(c))) if clusters else plt.cm.viridis\n",
    "    nrows, ncols = n_row_columns(len(set(c)))\n",
    "    \n",
    "    fig, ax = plt.subplots(1, figsize=(10, 8))\n",
    "    sc = plt.scatter(data[:,0], data[:,1], \n",
    "                     c=c, cmap=cmap,\n",
    "                     marker='o',\n",
    "                     alpha=alpha,\n",
    "                     s=size)\n",
    "    plt.setp(ax, xticks=[], yticks=[])\n",
    "    if clusters and show_centers:\n",
    "        # For each cluster, we add a text with name\n",
    "        for cluster in set(c):\n",
    "            cinds = np.flatnonzero(c == cluster)\n",
    "            cdata = data[cinds, :]\n",
    "            cluster_mean = cdata.mean(axis=0)\n",
    "            plt.text(cluster_mean[0], cluster_mean[1], str(cluster), \n",
    "                     horizontalalignment='center', size='large', color='black', weight='bold')            \n",
    "    if clusters:\n",
    "        cbar = plt.colorbar(boundaries=np.arange(len(set(c)) + 1) + min(c) - 0.5)\n",
    "        cbar.set_ticks(np.arange(n_clusters) + min(c))\n",
    "        cbar.set_ticklabels(list(set(c)))\n",
    "    else:\n",
    "        plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('UMAP on LSA normalized vs log10 coverate depth')\n",
    "plot_colored(umap_coords, np.log10(fulldf.sum()), size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('tSNE on LSA normalized vs', FACTOR, FACTORS_MAP)\n",
    "factors = [FACTOR_FUNCTION(bc) for bc in result_lsa.index]\n",
    "plot_colored(umap_coords, factors, clusters=True, size=20, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_row_columns(n):\n",
    "    nrows = max([r for r in range(1, math.ceil(math.sqrt(n))) if n % r == 0])\n",
    "    ncols = int(n / nrows)\n",
    "    return nrows, ncols\n",
    "\n",
    "    \n",
    "def plot_colored_split(data, c, size=10, alpha=0.5, contrast = 100):\n",
    "    \"\"\" Plot colored scatterplot split by factors\"\"\"\n",
    "    cmap = plt.cm.get_cmap('jet', len(set(c)))\n",
    "    nrows, ncols = n_row_columns(len(set(c)))\n",
    "    plt.subplots(nrows=nrows, ncols=ncols, figsize=(4 * ncols, 4 * nrows))\n",
    "    xmin, xmax = np.min(data[:,0]), np.max(data[:,0])\n",
    "    xlim = (xmin - 0.1 * (xmax - xmin), xmax + 0.1 * (xmax - xmin)) # +10% margin\n",
    "    ymin, ymax = np.min(data[:,1]), np.max(data[:,1])\n",
    "    ylim = (ymin - 0.1 * (ymax - ymin), ymax + 0.1 * (ymax - ymin)) # +10% margin\n",
    "    for i, f in enumerate(tqdm(set(c))):\n",
    "        ax = plt.subplot(nrows, ncols, i + 1)\n",
    "        plt.setp(ax, xticks=[], yticks=[])\n",
    "        plt.xlim(xlim)\n",
    "        plt.ylim(ylim)\n",
    "        plt.title(str(f))\n",
    "        # Plot slightly visible other as gray\n",
    "        nfdata = data[np.flatnonzero(np.logical_not(np.equal(c, f))), :]\n",
    "        plt.scatter(nfdata[:,0], nfdata[:,1], \n",
    "            color='gray',\n",
    "            marker='o',\n",
    "            alpha=alpha / contrast,\n",
    "            s=size)\n",
    "        # And factor data\n",
    "        fdata = data[np.flatnonzero(np.equal(c, f)), :]        \n",
    "        plt.scatter(fdata[:,0], fdata[:,1], \n",
    "                    color=cmap(i),\n",
    "                    marker='o',\n",
    "                    alpha=alpha,\n",
    "                    s=size)\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_colored_split(umap_coords, factors, size=5, alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph clustering of LSA normalized data\n",
    "\n",
    "Clustering should:\n",
    "* produce clusters enough clusters 10-15 recommended by Cell Ranger\n",
    "* too small clusters are likely artifacts < 100 cells\n",
    "* centers of clusters should be inside the cluster, otherwise we should increase number of clusters OR increase lower dimension space size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T15:33:34.733808Z",
     "start_time": "2019-04-25T15:33:31.262917Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "def graph_clustering(result, n_clusters):\n",
    "    # Ward method as default\n",
    "    print('Clustering', n_clusters)\n",
    "    clusters = AgglomerativeClustering(n_clusters=n_clusters).fit_predict(result).astype(int)\n",
    "    cluster_counter = Counter()\n",
    "    for c in clusters:\n",
    "        cluster_counter[c] += 1\n",
    "\n",
    "    # reorder clusters by size descending\n",
    "    clusters_reord = np.zeros(len(clusters), dtype=int)    \n",
    "    for i, (c, n) in enumerate(cluster_counter.most_common()):\n",
    "        clusters_reord[clusters == c] = i\n",
    "\n",
    "    return clusters_reord\n",
    "\n",
    "# 10-15 clusters is recommened values by Cell Ranger ATAC-Seq\n",
    "n_clusters = 15\n",
    "# Previous analysis\n",
    "# n_clusters = 10\n",
    "clusters = graph_clustering(result_lsa, n_clusters)\n",
    "print('Saving clusters to clusters{}.tsv'.format(n_clusters))\n",
    "clusters_df = pd.DataFrame({'cluster': clusters}, index=result_lsa.index)\n",
    "clusters_df.to_csv(OUTPUT_DIR + '/clusters{}.tsv'.format(n_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('QC clustermap of normalized LSA coordinates and clusters')\n",
    "n = 100\n",
    "print('Sampling', n, 'cells from each cluster')\n",
    "t = result_lsa # (barcodes x lsa coordinates) format\n",
    "cluster_dfs = []\n",
    "for cluster in tqdm(range(n_clusters)):\n",
    "    cluster_df = t.loc[clusters == cluster].sample(n=n, axis=0, replace=True)\n",
    "    cluster_df['cluster'] = cluster/n_clusters # Normalize cluster number to 0-1\n",
    "    cluster_dfs.append(cluster_df)\n",
    "t = pd.concat(cluster_dfs)\n",
    "\n",
    "sns.clustermap(t, col_cluster=False, row_cluster=False, figsize=(8, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusters_summary(df, value):\n",
    "    df_sum = df[['cluster', value]].groupby(['cluster']).sum().reset_index()\n",
    "    df_sum_age = df[['cluster', FACTOR, value]].groupby(['cluster', FACTOR]).sum().reset_index()\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    cmap=plt.cm.get_cmap('jet', len(df_sum))\n",
    "    sns.barplot(x=df_sum.index, y=df_sum[value],\n",
    "                palette=[cmap(i) for i in range(len(df_sum))])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print('Proportions')\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    cmap=plt.cm.get_cmap('jet', len(FACTORS_MAP))\n",
    "    \n",
    "    # Plot 1 - background - \"total\" \n",
    "    sns.barplot(x = df_sum.index, y = np.ones(len(df_sum)) * 100, color = cmap(0), alpha=0.5)\n",
    "    \n",
    "    percentages = np.zeros(len(df_sum))\n",
    "    for i, (k,v) in enumerate(FACTORS_MAP.items()):\n",
    "        if i > 0:\n",
    "            percentages += np.array(df_sum_age.loc[df_sum_age[FACTOR] == k][value]) / np.array(df_sum[value]) * 100.0\n",
    "            # Plot N - overlay - \"bottom\" series\n",
    "            bottom_plot = sns.barplot(x=df_sum.index, y=percentages, color = cmap(i), alpha=0.5)\n",
    "\n",
    "\n",
    "    legend_colors = [plt.Rectangle((0,0),1,1, fc=cmap(i), edgecolor = 'none') for i in range(len(FACTORS_MAP))]\n",
    "    legend_names = [v for _,v in FACTORS_MAP.items()]\n",
    "    l = plt.legend(legend_colors, legend_names, prop={'size':16})\n",
    "    l.draw_frame(False)\n",
    "\n",
    "    #Optional code - Make plot look nicer\n",
    "    sns.despine(left=True)\n",
    "    bottom_plot.set_ylabel('Cluster composition ' + FACTOR + ' %')\n",
    "    bottom_plot.set_xlabel('Cluster')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Computing summary for clusters')\n",
    "t = fulldf.T.copy()\n",
    "t[FACTOR] = [FACTOR_FUNCTION(bc) for bc in t.index]\n",
    "t['cluster'] = clusters\n",
    "t['counts'] = np.ones(len(t))\n",
    "\n",
    "print('Summary by', FACTOR, FACTORS_MAP)\n",
    "display(t[[FACTOR, 'counts']].groupby([FACTOR]).sum())\n",
    "\n",
    "clusters_summary(t, 'counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Summary UMI distribution per cluster')\n",
    "plt.figure(figsize=(10, 6))\n",
    "for c in tqdm(set(clusters)):\n",
    "    sns.kdeplot(np.log10(fulldf.T.iloc[np.flatnonzero(clusters == c)].sum(axis=1)), \n",
    "                label=str(c))\n",
    "\n",
    "plt.title('UMI summary coverage distribution')\n",
    "plt.xlabel('Summary coverage')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Peaks with non-zero coverage distribution per cluster')\n",
    "plt.figure(figsize=(10, 6))\n",
    "for c in tqdm(set(clusters)):\n",
    "    sns.kdeplot(np.log10(np.count_nonzero(fulldf.T.iloc[np.flatnonzero(clusters == c)], axis=1)), \n",
    "                label=str(c))\n",
    "\n",
    "plt.title('Peaks per cell')\n",
    "plt.xlabel('Log10 peaks')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clusters visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('UMAP on LSA normalized vs graph clusters')\n",
    "plot_colored(umap_coords, clusters, clusters=True, show_centers=True, size=20, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('UMAP on LSA normalized vs graph clusters')\n",
    "plot_colored_split(umap_coords, clusters, size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-SNE for dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "print('Processing t-SNE on L2-normalized data')\n",
    "tsne_coords = pd.DataFrame(TSNE(n_components=2).fit_transform(result_lsa.values),\n",
    "                           index=result_lsa.index,\n",
    "                           columns=['tsne1', 'tsne2'])\n",
    "print('t-SNE done!')\n",
    "# display(tsne_coords.head())\n",
    "\n",
    "print('Saving tSNE coordinates to tsne.tsv')\n",
    "tsne_coords.to_csv(OUTPUT_DIR + '/tsne.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('tSNE on LSA normalized vs log10 coverate depth')\n",
    "plot_colored(tsne_coords.values, np.log10(fulldf.sum()), size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('tSNE on LSA normalized vs', FACTOR, FACTORS_MAP)\n",
    "factors = np.array([FACTOR_FUNCTION(bc) for bc in result_lsa.index])\n",
    "plot_colored(tsne_coords.values, factors, clusters=True, size=20, alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_colored_split(tsne_coords.values, factors, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T15:33:43.836235Z",
     "start_time": "2019-04-25T15:33:34.736053Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('tSNE on LSA normalized vs graph clusters')\n",
    "plot_colored(tsne_coords.values, clusters, clusters=True, show_centers=True, size=20, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_colored_split(tsne_coords.values, clusters, size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marker regions analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_z(xs, ys, zs, ax, title, xlabel, ylabel, alpha=0.5, size=10):\n",
    "    \"\"\"Plot zscores scatter plot\"\"\"\n",
    "    plt.setp(ax, xticks=[], yticks=[])\n",
    "    zscores_pos = zs > 0\n",
    "    zscores_pos_ind = np.flatnonzero(zscores_pos)\n",
    "    zscores_neg_ind = np.flatnonzero(np.logical_not(zscores_pos))\n",
    "\n",
    "    pos_colors = np.zeros((len(zscores_pos_ind), 4))\n",
    "    pos_colors[:, 0] = 1.0 # red\n",
    "    pos_colors[:, 3] = alpha * zscores[zscores_pos_ind] / np.max(zscores) # alpha\n",
    "\n",
    "    neg_colors = np.zeros((len(zscores_neg_ind), 4))\n",
    "    neg_colors[:, 2] = 1.0 # blue\n",
    "    neg_colors[:, 3] = alpha * zscores[zscores_neg_ind] / np.min(zscores) # alpha\n",
    "\n",
    "    ax.scatter(xs[zscores_neg_ind], ys[zscores_neg_ind], c=neg_colors, s=size)\n",
    "    ax.scatter(xs[zscores_pos_ind], ys[zscores_pos_ind], c=pos_colors, s=size)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Z-scores for REGIONS on interest')\n",
    "peaks = normdf.index[np.flatnonzero(peaks_filter)]\n",
    "\n",
    "nrows, ncols = n_row_columns(len(marker_regions_peaks))\n",
    "plt.subplots(nrows=nrows, ncols=ncols, figsize=(4 * ncols, 4 * nrows))\n",
    "for i, (k, r) in enumerate(tqdm(marker_regions_peaks.items())):\n",
    "    c,s,e = r\n",
    "    peak = f'{c}:{s}-{e}'    \n",
    "    # Z scale for each marker peak across all cells\n",
    "    zscores = zscore(normdf.loc[peak].T)\n",
    "    t1 = tsne_coords['tsne1']\n",
    "    t2 = tsne_coords['tsne2']\n",
    "    ax = plt.subplot(nrows, ncols, i + 1)\n",
    "    plot_z(t1, t2, zscores, ax, title=k, xlabel='tSNE axis 1', ylabel='tSNE axis 2', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BigWig RPM normalized profiles\n",
    "\n",
    "**Pipeline** Split UMI reads by clusters and use RPKM normalization.\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Seurat** We created pseudo-bulk ATAC-seq profiles by pooling together cells with for each cell type. Each cell type showed enriched accessibility near canonical marker genes. Chromatin accessibility tracks are normalized to sequencing depth (RPKM normalization) in each pooled group.\n",
    "\n",
    "**SnapATAC** Next we aggregate cells from the each cluster to create an ensemble track for peak calling and visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading full fragments dataframe')\n",
    "fdf = pd.read_csv(fragments_file, sep='\\t', names=['chr', 'start', 'end', 'barcode', 'reads'])\n",
    "\n",
    "print('Aggregating fragments and clusters')\n",
    "df_for_bigwigs = pd.merge(left=fdf, right=clusters_df,\n",
    "                          left_on='barcode', right_on=clusters_df.index)\n",
    "display(df_for_bigwigs.head()) # already sorted by chromosomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing BigWig files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pyBigWig\n",
    "\n",
    "step = 100\n",
    "bigwigs_path = OUTPUT_DIR + '/bigwig'\n",
    "! mkdir -p {bigwigs_path}\n",
    "    \n",
    "for cluster in set(clusters):\n",
    "    bw_path = bigwigs_path + '/cluster_{}.bw'.format(cluster)\n",
    "    if os.path.exists(bw_path):\n",
    "        print(f'Bigwig file {bw_path} exists, skipping...')\n",
    "        continue\n",
    "    print('Processing cluster', cluster, bw_path)\n",
    "    df_for_bigwigs_cluster = df_for_bigwigs.loc[df_for_bigwigs['cluster'] == cluster]\n",
    "    chr_lengths = df_for_bigwigs_cluster[['chr', 'end']].groupby('chr').max().reset_index()\n",
    "\n",
    "    with pyBigWig.open(bw_path, 'w') as bw:\n",
    "        bw.addHeader(list(zip(chr_lengths['chr'], chr_lengths['end'])))\n",
    "        for chromosome in tqdm(chr_lengths['chr']):\n",
    "            df_for_bigwigs_cluster_chr =\\\n",
    "                df_for_bigwigs_cluster.loc[df_for_bigwigs_cluster['chr'] == chromosome].sort_values(['start', 'end'])\n",
    "\n",
    "            starts = list(df_for_bigwigs_cluster_chr['start'])\n",
    "            ends = list(df_for_bigwigs_cluster_chr['end'])\n",
    "            reads = list(df_for_bigwigs_cluster_chr['reads'])\n",
    "            chr_length = int(chr_lengths.loc[chr_lengths['chr'] == chromosome]['end'])\n",
    "            values = np.zeros(int(math.floor(chr_length / step)) + 1)\n",
    "\n",
    "            for i in range(len(df_for_bigwigs_cluster_chr)):\n",
    "                # Ignore PCR duplicates here!\n",
    "                # v = reads[i] \n",
    "                v = 1.0\n",
    "                si = int(math.floor(starts[i] / step)) \n",
    "                ei = int(math.floor(ends[i] / step)) \n",
    "                if ei == si:\n",
    "                    values[si] += v\n",
    "                else:\n",
    "                    values[si] += v / 2 \n",
    "                    values[ei] += v / 2\n",
    "            \n",
    "            non_zero = values > 0\n",
    "            non_zero_inds = np.flatnonzero(non_zero)\n",
    "            starts_np = non_zero_inds * step\n",
    "            ends_np = (non_zero_inds + 1) * step\n",
    "            values_np = values[non_zero]\n",
    "            # RPM normalization\n",
    "            values_np = values_np * (1000000.0 / sum(values_np))\n",
    "            chroms_np = np.array([chromosome] * sum(non_zero))\n",
    "            bw.addEntries(chroms_np, starts=starts_np, ends=ends_np, values=values_np)                     \n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closest genes annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Preprocess GTF file')\n",
    "\n",
    "gtf_file = GTF_FILE\n",
    "transcripts_bed = gtf_file + '.bed'\n",
    "if not os.path.exists(transcripts_bed):\n",
    "    GTF_TO_BED_SH = \"\"\"\n",
    "TNF=$(cat $1 | grep transcript_id | grep -v '#' | head -n 1 | awk '{for (i=4; i<NF; i++) {if ($3==\"transcript\" && $i==\"gene_name\") print (i+1)}}')\n",
    "cat $1 | awk -v TNF=$TNF -v OFS=\"\\t\" '{if ($3==\"transcript\") {print $1,$4-1,$5,$TNF,0,$7}}' | tr -d '\";' | sort -k1,1 -k2,2n -k3,3n --unique > $2\n",
    "    \"\"\"\n",
    "    with tempfile.NamedTemporaryFile(prefix='gtf_to_bed', suffix='.sh', delete=False) as f:\n",
    "        f.write(GTF_TO_BED_SH.encode('utf-8'))\n",
    "        f.close()\n",
    "        ! bash {f.name} {gtf_file} {transcripts_bed} \n",
    "print('Transcripts bed file', transcripts_bed)\n",
    "\n",
    "transcripts_tss = gtf_file + '.tss'\n",
    "if not os.path.exists(transcripts_tss):\n",
    "    BED_TO_TSS_SH = \"\"\"\n",
    "cat $1 | awk -v OFS=\"\\t\" '{if ($6==\"+\") {print $1, $2, $2+1, $4, 0, \"+\"} else {print $1,$3-1,$3, $4, 0, \"-\"}}' | sort -k1,1 -k2,2n -k3,3n --unique > $2\n",
    "    \"\"\"\n",
    "    with tempfile.NamedTemporaryFile(prefix='bed_to_tss', suffix='.sh', delete=False) as f:\n",
    "        f.write(BED_TO_TSS_SH.encode('utf-8'))\n",
    "        f.close()\n",
    "        ! bash {f.name} {transcripts_bed} {transcripts_tss} \n",
    "print('Transcripts tss file', transcripts_tss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline** Preprocess GENES to TSS and use `bedtools closest -D b` to report distance to TSS, upstream or downstream w.r.t. the gene strand.\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Cell Ranger ATAC** We use `bedtools closest -D b` to associate each peak with genes based on closest transcription start sites (packaged within the reference) such that the peak is within 1000 bases upstream or 100 bases downstream of the TSS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locations_closest_genes(locations, transcripts_tss):\n",
    "    print('Annotate list of locations in format \"chr:start-end\" with closest tss of genes')\n",
    "    with tempfile.NamedTemporaryFile(delete=False) as f:\n",
    "        # Save index to restore original order after sorting\n",
    "        for i, t in enumerate([re.split(':|-', p) for p in locations]):\n",
    "            f.write('{}\\t{}\\t{}\\t{}\\n'.format(*t, i).encode('utf-8'))\n",
    "        f.close()\n",
    "        closest_file = f.name + '.closest'\n",
    "        sorted_file = f.name + '.sorted'\n",
    "        ! sort -k1,1 -k2,2n {f.name} > {sorted_file}\n",
    "        ! bedtools closest -a {sorted_file} -b {transcripts_tss} -D b > {closest_file}\n",
    "        closest_df = pd.read_csv(\n",
    "            closest_file, sep='\\t', \n",
    "            names=['chr', 'start', 'end', 'index', \n",
    "                   'gene_chr', 'gene_start', 'gene_end', 'gene', 'gene_score', 'gene_strand', 'distance'])\n",
    "        # Restore original sorting as \n",
    "        closest_df.sort_values(by=['index'], inplace=True)\n",
    "        # Pick only first closest gene in case of several\n",
    "        closest_df.drop_duplicates(['index'], inplace=True)\n",
    "        return closest_df[['chr', 'start', 'end', 'gene', 'distance']]\n",
    "\n",
    "\n",
    "closest_genes = locations_closest_genes(list(normdf.index), transcripts_tss)\n",
    "# Restore 'peak' to be able to merge on it\n",
    "closest_genes['peak'] = closest_genes['chr'] +\\\n",
    "                        ':' + closest_genes['start'].astype(str) +\\\n",
    "                        '-' + closest_genes['end'].astype(str)\n",
    "display(closest_genes.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save cluster mean values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose to (barcodes x peaks) format\n",
    "t = normdf.T \n",
    "\n",
    "# Per cluster mean values for peaks\n",
    "clusters_means = {str(c): t.loc[clusters == c].mean() for c in set(clusters)}\n",
    "clusters_means_df = pd.DataFrame(clusters_means).reset_index()[[str(c) for c in set(clusters)]]\n",
    "clusters_means_genes_df = pd.concat([closest_genes.reset_index(), clusters_means_df], axis=1).drop(columns=['index'])\n",
    "# Save peak without dashes or colons\n",
    "clusters_means_genes_df['peak'] = [re.sub('[:-]', '_', p) for p in clusters_means_genes_df['peak']]\n",
    "\n",
    "print('Saving clusters peaks_means to clusters_peaks_values.tsv')\n",
    "display(clusters_means_genes_df.head())\n",
    "clusters_means_genes_df.to_csv(OUTPUT_DIR + '/clusters_peaks_values.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Hierarchical clustering of clusters mean values')\n",
    "sns.clustermap(clusters_means_genes_df[[str(i) for i in set(clusters)]], \n",
    "               col_cluster=True, row_cluster=False, figsize=(10, 5), cmap=plt.cm.viridis)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differential peaks analysis (Markers)\n",
    "\n",
    "**Pipeline**\n",
    "\n",
    "Use Cell Ranger ATAC to get differential markers. \n",
    "\n",
    "<hr>\n",
    "\n",
    "**Cell Ranger ATAC**\n",
    "For each peak we perform test cluster vs others, so that several clusters may yield significant results vs others, we take into account only the cluster with greatest mean value.\\\n",
    "Test is is perfromed using Cell Ranger default testing procedure based on Negative Binomial GLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch 10x Genomics Cell Ranger DE\n",
    "from diffexp import *\n",
    "\n",
    "def run_10x_differential_expression(df, clusters, fdr=0.05):\n",
    "    \"\"\" Compute differential expression for each cluster vs all other cells\n",
    "        Args: df          - feature expression data (peak x barcode)\n",
    "              clusters    - 0-based cluster labels\n",
    "              fdr         - false discovery rate\"\"\"\n",
    "\n",
    "    peaks = list(df.index)\n",
    "    distinct_clusters = set(clusters)\n",
    "    print('Detected clusters', len(distinct_clusters))\n",
    "\n",
    "    print(\"Computing params...\")\n",
    "    m = df.values\n",
    "    sseq_params = compute_sseq_params(m)\n",
    "    \n",
    "    cluster_markers = []\n",
    "    for cluster in distinct_clusters:\n",
    "        in_cluster = clusters == cluster\n",
    "        group_a = np.flatnonzero(in_cluster)\n",
    "        group_b = np.flatnonzero(np.logical_not(in_cluster))\n",
    "        print('Computing DE for cluster {}...'.format(cluster))\n",
    "\n",
    "        de_result = sseq_differential_expression(m, group_a, group_b, sseq_params)\n",
    "        de_result['cluster'] = [cluster] * len(de_result)\n",
    "        de_result['peak'] = peaks\n",
    "        passed_fdr = de_result['adjusted_p_value'] < fdr\n",
    "        passed_log2_fc = de_result['log2_fold_change'] > 0\n",
    "        # Filter out different by statistical tests with log2 fold change > 0\n",
    "        passed = np.logical_and(passed_fdr, passed_log2_fc)\n",
    "        print('DE:', sum(passed), 'of', len(de_result), \n",
    "              'FDR:', sum(passed_fdr), 'log2fc>0:', sum(passed_log2_fc))\n",
    "\n",
    "        passed_de = de_result.loc[passed]\n",
    "        cluster_de_markers = passed_de[\n",
    "            ['cluster', 'peak', 'norm_mean_a', 'norm_mean_b', 'log2_fold_change', 'p_value', 'adjusted_p_value']\n",
    "        ].rename(columns={'norm_mean_a': 'norm_mean_cluster', 'norm_mean_b': 'norm_mean_others'}) \n",
    "\n",
    "        cluster_markers.append(cluster_de_markers)    \n",
    "\n",
    "    return pd.concat(cluster_markers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Cell Ranger ATAC like differential analysis\n",
    "# Each peak is tested independently, no normalization to peak length required.\n",
    "markers = run_10x_differential_expression(fulldf.loc[peaks_filter], clusters)\n",
    "print('Total markers', len(markers))\n",
    "# display(markers.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markers summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of differential markers per cluster')\n",
    "t = markers[['cluster']].copy()\n",
    "t['count'] = 1\n",
    "display(pd.pivot_table(t, values='count', index=['cluster'], aggfunc=np.sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotate markers with genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markers_with_genes = pd.merge(left=markers[['cluster', 'norm_mean_cluster', 'norm_mean_cluster', \n",
    "                                            'log2_fold_change', 'p_value', 'adjusted_p_value', 'peak']],\n",
    "                         right=closest_genes,\n",
    "                         left_on='peak',\n",
    "                         right_on='peak')\n",
    "\n",
    "# Save peak without dashes or colons\n",
    "markers_with_genes['peak'] = [re.sub('[:-]', '_', p) for p in markers_with_genes['peak']]\n",
    "\n",
    "# rearrange columns, sort\n",
    "markers_with_genes = markers_with_genes[['chr', 'start', 'end', 'peak',\n",
    "                                         'cluster', 'norm_mean_cluster', 'norm_mean_cluster', \n",
    "                                         'log2_fold_change', 'p_value', 'adjusted_p_value', \n",
    "                                         'gene', 'distance']].sort_values(by=['cluster', 'adjusted_p_value'])\n",
    "# display(markers_with_genes.head())\n",
    "\n",
    "print('Saving all the markers', len(markers_with_genes), 'to markers.tsv')\n",
    "markers_with_genes.to_csv(OUTPUT_DIR+ '/markers.tsv', sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing top markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_markers(df, n_clusters=n_clusters, top=100):\n",
    "    top_markers = pd.concat([\n",
    "        df.loc[df['cluster'] == c].sort_values(by=['adjusted_p_value']).head(top) \n",
    "                    for c in range(0, n_clusters)])\n",
    "    top_markers['peak'] = top_markers['chr'] +\\\n",
    "                          ':' + top_markers['start'].astype(str) + '-'\\\n",
    "                          + top_markers['end'].astype(str)\n",
    "    return top_markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Computing Z scores for mean values in cluster for 50 top markers')\n",
    "top_markers = get_top_markers(markers_with_genes, top=50)\n",
    "t = normdf.T # Transpose to (barcodes x peaks) format\n",
    "t = t[list(top_markers['peak'])] # top markers only\n",
    "\n",
    "t = pd.concat([pd.DataFrame(t.loc[clusters == cluster].mean()).T for cluster in set(clusters)])\n",
    "t.index = range(n_clusters)\n",
    "\n",
    "# Transform to Z-score\n",
    "for c in t.columns:\n",
    "    t[c] = zscore(t[c])\n",
    "\n",
    "print('Clustermap of Z scores')\n",
    "sns.clustermap(t.T, col_cluster=False, row_cluster=False, figsize=(10, 10), cmap=plt.cm.seismic)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top markers visualization on T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T15:37:58.681824Z",
     "start_time": "2019-04-25T15:37:54.162643Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('t-SNE based Z-SCORE visualizations for markers, mean z-score for each cell is given')\n",
    "t = normdf\n",
    "\n",
    "nrows, ncols = n_row_columns(n_clusters)\n",
    "plt.subplots(nrows=nrows, ncols=ncols, figsize=(4 * ncols, 4 * nrows))\n",
    "for i, cluster in enumerate(tqdm(set(clusters))):\n",
    "    # Table (marker for cluster x cells)\n",
    "    markers_df = t.loc[top_markers.loc[top_markers['cluster'] == cluster]['peak']].T.copy()\n",
    "\n",
    "    # Z scale for each marker peak across all cells\n",
    "    for c in markers_df.columns:\n",
    "        markers_df[c] = zscore(markers_df[c])\n",
    "    \n",
    "    # Average Z score for each cell\n",
    "    zscores = markers_df.T.mean()\n",
    "    t1 = tsne_coords['tsne1']\n",
    "    t2 = tsne_coords['tsne2']\n",
    "    ax = plt.subplot(nrows, ncols, i + 1)\n",
    "    plot_z(t1, t2, zscores, ax, title=f'Cluster {cluster}', xlabel='tSNE axis 1', ylabel='tSNE axis 2')        \n",
    "#     plt.setp(ax, xticks=[], yticks=[])    \n",
    "#     sc = ax.scatter(t1, t2, s=5, c=zscores, cmap=plt.cm.seismic, edgecolor='none', alpha=0.5)\n",
    "#     plt.colorbar(sc)\n",
    "#     plt.title(f'Cluster {cluster}')\n",
    "#     plt.xlabel('tSNE axis 1')\n",
    "#     plt.ylabel('tSNE axis 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save differential markers to BED\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T15:39:39.518557Z",
     "start_time": "2019-04-25T15:39:38.869915Z"
    }
   },
   "outputs": [],
   "source": [
    "cluster_peaks_path = os.path.join(OUTPUT_DIR, 'markers')\n",
    "! mkdir -p {cluster_peaks_path}\n",
    "print('Cleanup peaks_clusters', cluster_peaks_path)\n",
    "for f in glob.glob('{}/markers_*.bed'.format(cluster_peaks_path)):\n",
    "    os.remove(f)\n",
    "\n",
    "mlap_max = (-np.log10(markers.loc[markers['adjusted_p_value'] != 0]['adjusted_p_value'])).max()\n",
    "print('BED scoring as -log10 adjusted pval')\n",
    "print('Max of -log10 adjusted pval', mlap_max)\n",
    "\n",
    "for c in set(clusters):\n",
    "    bed_file = '{}/markers_{}.bed'.format(cluster_peaks_path, c)\n",
    "    markers_cluster = markers.loc[markers['cluster'] == c].sort_values(\n",
    "        ['log2_fold_change'], ascending=False).iloc[::-1]\n",
    "    markers_cluster.index = range(len(markers_cluster))\n",
    "    print('Writing cluster', c, 'differential peaks to', bed_file)\n",
    "    with open(bed_file, 'w') as bed:\n",
    "        for i in range(len(markers_cluster)):\n",
    "            peak = markers_cluster['peak'][i]\n",
    "            ap = markers_cluster['adjusted_p_value'][i]\n",
    "            mlap = -np.log10(ap) if ap != 0.0 else mlap_max\n",
    "            line = '{}\\t{}\\t{}\\t.\\n'.format(\n",
    "                re.sub(':|-', '\\t', peak), \n",
    "                c,\n",
    "                mlap)\n",
    "            bed.write(line)\n",
    "print('Done')                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation data for single cell explorer\n",
    "Preprocess data for single cell explorer http://artyomovlab.wustl.edu/shiny/single_cell_explorer\n",
    "\n",
    "Upload `data_for_plot.tsv`, `expData.Rda` and `markers.tsv` to corresponding single cell explorer folder and voila!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import io, sparse\n",
    "sce_path = os.path.join(OUTPUT_DIR, 'sce')\n",
    "! mkdir -p {sce_path}\n",
    "\n",
    "print('Save data_for_plot.tsv')\n",
    "barcodes = normdf.T.index.values\n",
    "data_for_plot = pd.DataFrame(\n",
    "    {'tSNE_1': tsne_coords['tsne1'], \n",
    "     'tSNE_2': tsne_coords['tsne2'], \n",
    "     'Cluster': clusters,\n",
    "     'nCounts': fulldf.sum()})\n",
    "data_for_plot.index = barcodes\n",
    "data_for_plot.to_csv(os.path.join(sce_path, 'data_for_plot.tsv'), sep='\\t')\n",
    "\n",
    "print('Save RPM expData.Rda.mtx')\n",
    "csc = sparse.csc_matrix(normdf.values)\n",
    "io.mmwrite(os.path.join(sce_path, 'expData.Rda.mtx'), csc)\n",
    "\n",
    "print('Save barcodes.txt')\n",
    "with open(os.path.join(sce_path, 'barcodes.txt'), 'w') as f:\n",
    "    for b in normdf.columns.values:\n",
    "        f.write(b + '\\n')\n",
    "\n",
    "print('Save peaks.txt')\n",
    "with open(os.path.join(sce_path, 'peaks.txt'), 'w') as f:\n",
    "    for p in normdf.index.values:\n",
    "        # Save peak without dashes or colons        \n",
    "        f.write(re.sub('[:-]', '_', p) + '\\n')\n",
    "\n",
    "print('Save markers.tsv')\n",
    "markers_with_genes.to_csv(os.path.join(sce_path, 'markers.tsv'), index=None, sep='\\t')\n",
    "        \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Converting to Rda format in R')\n",
    "\n",
    "R_CODE = \"\"\"\n",
    "require(readr)\n",
    "require(Matrix)\n",
    "library(magrittr)\n",
    "\n",
    "barcodes <- readr::read_tsv('{0}/barcodes.txt', col_names = F)\n",
    "peaks <- readr::read_tsv('{0}/peaks.txt', col_names = F)\n",
    "expData <- Matrix::readMM('{0}/expData.Rda.mtx') %>%\n",
    "    magrittr::set_rownames(peaks$X1) %>%\n",
    "    magrittr::set_colnames(barcodes$X1)\n",
    "save(expData, file = \"{0}/expData.Rda\")\n",
    "\"\"\".format(sce_path)\n",
    "\n",
    "with tempfile.NamedTemporaryFile(prefix='prepare', suffix='.R', delete=False) as f:\n",
    "    f.write(R_CODE.encode('utf-8'))\n",
    "    f.close()\n",
    "    ! Rscript {f.name}\n",
    "print('Cleanup')\n",
    "os.remove('{}/barcodes.txt'.format(sce_path))\n",
    "os.remove('{}/peaks.txt'.format(sce_path))\n",
    "os.remove('{}/expData.Rda.mtx'.format(sce_path))\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Original 10x Cell Ranger results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters10xpath = CLUSTERS_FILE\n",
    "clusters10xdf = pd.read_csv(clusters10xpath, sep=',')\n",
    "clusters10xdf['Cluster'] = clusters10xdf['Cluster'].astype(int) - 1 # Clusters start with 1 in 10x\n",
    "clusters10xdf[FACTOR] = [FACTOR_FUNCTION(bc) for bc in clusters10xdf['Barcode']]\n",
    "# display(clusters10xdf.head())\n",
    "print('Total clusters', len(set(clusters10xdf['Cluster'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Size of clusters analysis')\n",
    "t = clusters10xdf.copy()\n",
    "t['counts'] = np.ones(len(t))\n",
    "t.rename(columns={'Cluster': 'cluster'}, inplace=True)\n",
    "t.index = t['Barcode']\n",
    "print('Summary by', FACTOR, FACTORS_MAP)\n",
    "display(t[[FACTOR, 'counts']].groupby([FACTOR]).sum())\n",
    "\n",
    "clusters_summary(t[['cluster', 'counts', FACTOR]], 'counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cell Ranger ATAC original T-SNE visualization')\n",
    "tsne10xpath = TSNE_FILE\n",
    "tsne10xdf = pd.read_csv(tsne10xpath, sep=',')\n",
    "\n",
    "mergeddf = pd.merge(left=tsne10xdf,\n",
    "                    right=clusters10xdf, \n",
    "                    left_on='Barcode', right_on='Barcode')\n",
    "\n",
    "plot_colored(mergeddf[['TSNE-1', 'TSNE-2']].values, mergeddf['Cluster'], \n",
    "             clusters=True,\n",
    "             show_centers=True,\n",
    "             size=20, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('tSNE on LSA normalized vs', FACTOR, FACTORS_MAP)\n",
    "plot_colored(mergeddf[['TSNE-1', 'TSNE-2']].values, clusters10xdf[FACTOR], clusters=True, size=20, alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering comparison vs 10x Cell Ranger ATAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('sc-atacseq-explorer clusters into Cell Ranger ATAC coordinates') \n",
    "mergeddf = pd.merge(pd.DataFrame({'Barcode': tsne_coords.index, 'Cluster': clusters}),\n",
    "                    right=tsne10xdf, \n",
    "                    left_on='Barcode', right_on='Barcode')\n",
    "\n",
    "print('tSNE on LSA normalized vs Cell Ranger ATAC clusters')\n",
    "plot_colored(mergeddf[['TSNE-1', 'TSNE-2']].values, \n",
    "             mergeddf['Cluster'], \n",
    "             clusters=True,\n",
    "             show_centers=True,\n",
    "             size=20, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Clusters from Cell Ranger ATAC into sc-atacseq-explorer coordinates') \n",
    "mergeddf = pd.merge(pd.DataFrame({'Barcode': tsne_coords.index,\n",
    "                                 'tsne1': tsne_coords['tsne1'],\n",
    "                                 'tsne2': tsne_coords['tsne2']}),\n",
    "                    right=clusters10xdf, \n",
    "                    left_on='Barcode', right_on='Barcode')\n",
    "\n",
    "print('tSNE on LSA normalized vs Cell Ranger ATAC clusters')\n",
    "plot_colored(mergeddf[['tsne1', 'tsne2']].values, \n",
    "             mergeddf['Cluster'], \n",
    "             clusters=True,\n",
    "             show_centers=True,\n",
    "             size=20, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "840px",
    "left": "0px",
    "right": "1228px",
    "top": "110px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
